{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, normalize, RobustScaler\n",
    "from scipy import sparse\n",
    "import sys\n",
    "\n",
    "# Get the current working directory of the Jupyter notebook\n",
    "notebook_directory = os.getcwd()\n",
    "# Assuming the notebook is in the 'bin/' folder, add the parent directory to sys.path\n",
    "parent_directory = os.path.dirname(notebook_directory)\n",
    "sys.path.append(parent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "cache_file = Path(\"movie_synopsis_cache.json\")\n",
    "\n",
    "# Function to load cache data from a file\n",
    "def load_cache():\n",
    "    if cache_file.is_file() and cache_file.stat().st_size > 0:\n",
    "        with open(cache_file, 'r') as file:\n",
    "            try:\n",
    "                return json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                return {}\n",
    "    return {}\n",
    "\n",
    "# Function to save cache data to a file\n",
    "def save_cache(cache):\n",
    "    with open(cache_file, 'w') as file:\n",
    "        json.dump(cache, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from imdb import Cinemagoer\n",
    "# Create an instance of the Cinemagoer class\n",
    "cg = Cinemagoer()\n",
    "\n",
    "# Create methods to fetch movie details given a list of imdb movie ids\n",
    "def get_movie_details(imdb_id):\n",
    "    cache = load_cache()\n",
    "\n",
    "    # Check if the movie data is in cache\n",
    "    if imdb_id in cache:\n",
    "        print(\"Retrieved from cache.\")\n",
    "        return cache[imdb_id]\n",
    "\n",
    "    # If not in cache, get movie data\n",
    "    start = timeit.default_timer()\n",
    "    cg_imdb_id = imdb_id.replace(\"tt\", \"\")\n",
    "    movie = cg.get_movie(cg_imdb_id)\n",
    "    end = timeit.default_timer()\n",
    "    print(\"get_movie_details took {} seconds to run\".format(end - start))\n",
    "    result = {}\n",
    "\n",
    "    keys = [\"title\", \"genres\", \"runtimes\", \"original air date\", \"rating\", \"votes\", \"imdbID\", \"language codes\", \"year\", \"director\", \"cast\"]\n",
    "    for key in keys:\n",
    "        if key not in movie:\n",
    "            result[key] = None\n",
    "        elif key == \"cast\":\n",
    "            result[key] = [c.personID for c in movie[key][:5]]\n",
    "        elif key == \"director\":\n",
    "            result[key] = [c.personID for c in movie[key]]\n",
    "        else:\n",
    "            result[key] = movie.get(key, None)\n",
    "\n",
    "    synopsis_present = True if \"synopsis\" in movie and len(movie[\"synopsis\"]) > 0 else False\n",
    "    plot_present = True if \"plot\" in movie and len(movie[\"plot\"]) > 0 else False\n",
    "    if synopsis_present and plot_present:\n",
    "        result[\"synopsis\"] = movie[\"synopsis\"][0]\n",
    "        result[\"plot\"] = movie[\"plot\"][0]\n",
    "    elif synopsis_present:\n",
    "        result[\"synopsis\"] = movie[\"synopsis\"][0]\n",
    "        result[\"plot\"] = movie[\"synopsis\"][0]\n",
    "    elif plot_present:\n",
    "        result[\"synopsis\"] = movie[\"plot\"][0]\n",
    "        result[\"plot\"] = movie[\"plot\"][0]\n",
    "    else:\n",
    "        result[\"synopsis\"] = \"\"\n",
    "        result[\"plot\"] = \"\"\n",
    "\n",
    "    # Save the new data to cache\n",
    "    print(\"trying to save \", imdb_id)\n",
    "    cache[imdb_id] = result\n",
    "    save_cache(cache)\n",
    "    return result\n",
    "\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(1))  # Retries up to 3 times with a 1-second wait between tries\n",
    "def get_movie_details_with_retry(movie):\n",
    "    return get_movie_details(movie)\n",
    "\n",
    "def get_movie_details_as_data_frame(movie_list):\n",
    "    all_movie_details = {}\n",
    "    for movie in movie_list:\n",
    "        all_movie_details[movie] = get_movie_details_with_retry(movie)\n",
    "    all_movie_details = [all_movie_details[movie] for movie in movie_list if movie in all_movie_details]\n",
    "    return pd.json_normalize(all_movie_details)\n",
    "\n",
    "print(get_movie_details_as_data_frame([\"tt6166392\", \"tt4046784\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Given a date range, fetch all the movies that were released during that period.\n",
    "# Additional filters like language/minimum vote count can also be specified\n",
    "MINIMUM_VOTE_COUNT = 200\n",
    "LANGUAGES = [\"en\"]\n",
    "\n",
    "def get_tmdb_movies_in_range(start, end):\n",
    "    api_key = '0b2cc6b5655e6c00206bd71118d1156f'\n",
    "    languages = \",\".join(LANGUAGES)\n",
    "    url = f'https://api.themoviedb.org/3/discover/movie?api_key={api_key}&primary_release_date.gte={start}&primary_release_date.lte={end}&include_adult=false&include_video=false&with_original_language={languages}&page=1&sort_by=popularity.desc&vote_count.gte={MINIMUM_VOTE_COUNT}'\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    total_pages = data[\"total_pages\"]\n",
    "    total_results = data[\"total_results\"]\n",
    "    movies_in_date_range = []\n",
    "    print(f\"total_results: {total_results}\")\n",
    "\n",
    "    for page in range(total_pages):\n",
    "        try:\n",
    "            url = f'https://api.themoviedb.org/3/discover/movie?api_key={api_key}&primary_release_date.gte={start}&primary_release_date.lte={end}&include_adult=false&include_video=false&with_original_language={languages}&page={page+1}&sort_by=popularity.desc'\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "            movies_in_date_range.extend(data[\"results\"])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(.1)\n",
    "    print(f\"total_results extracted: {len(movies_in_date_range)}\")\n",
    "    return movies_in_date_range\n",
    "\n",
    "def get_imdb_ids_for_tmdb_movies_in_range(start, end):\n",
    "    api_key = '0b2cc6b5655e6c00206bd71118d1156f'\n",
    "\n",
    "    movies = get_tmdb_movies_in_range(start, end)\n",
    "    imdb_ids = []\n",
    "    found_movies = []\n",
    "    low_votes_movies = []\n",
    "    missing_movies = []\n",
    "    for movie in movies:\n",
    "        try:\n",
    "            id = movie[\"id\"]\n",
    "            url = f\"https://api.themoviedb.org/3/movie/{id}/external_ids?api_key={api_key}\"\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "            imdb_id = data[\"imdb_id\"]\n",
    "            if imdb_id is not None:\n",
    "                if int(movie[\"vote_count\"]) >= MINIMUM_VOTE_COUNT:\n",
    "                    imdb_ids.append(imdb_id)\n",
    "                    found_movies.append((id, movie[\"original_title\"], movie[\"vote_count\"]))\n",
    "                else:\n",
    "                    low_votes_movies.append((id, movie[\"original_title\"], movie[\"vote_count\"]))\n",
    "            else:\n",
    "                missing_movies.append((id, movie[\"original_title\"], movie[\"vote_count\"]))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(1)\n",
    "    print(f\"Number of imdb ids extracted: {len(imdb_ids)}\")\n",
    "    print(f\"Missing movies: {missing_movies}\")\n",
    "    print(f\"Low votes movies: {low_votes_movies}\")\n",
    "    print(f\"Found movies: {found_movies}\")\n",
    "    return imdb_ids\n",
    "\n",
    "yesterday = (datetime.now() - timedelta(1)).strftime('%Y-%m-%d') # Eventually, we will use this in the cron job that runs to populate for the last 'n' days\n",
    "start = \"2023-01-01\"\n",
    "end = \"2023-12-31\"\n",
    "imdb_movie_ids = get_imdb_ids_for_tmdb_movies_in_range(start, end)\n",
    "\n",
    "print(imdb_movie_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each movie discovered, fetch full details using Cinemagoer\n",
    "movie_details_df = get_movie_details_as_data_frame(imdb_movie_ids)\n",
    "titles_with_synopsis = movie_details_df['title'].tolist()\n",
    "synopsis_list = movie_details_df['synopsis'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "\n",
    "# Initialize tokenizer and model from pre-trained BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to create embeddings for a list of synopses using BERT\n",
    "def get_bert_embeddings(synopses):\n",
    "    embeddings = []\n",
    "    num_processed = 0\n",
    "    for synopsis in synopses:\n",
    "        # Tokenize the synopsis and convert to input format expected by BERT\n",
    "        inputs = tokenizer(synopsis, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        # Get the output from BERT model\n",
    "        outputs = model(**inputs)\n",
    "        # Use the mean of the last hidden state as the embedding\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n",
    "        embeddings.append(embedding)\n",
    "        \n",
    "        num_processed = num_processed + 1\n",
    "        print(f'{len(synopses) - num_processed} remaining...')\n",
    "    return embeddings\n",
    "\n",
    "# Create BERT embeddings for the synopses\n",
    "bert_embeddings = get_bert_embeddings(synopsis_list)\n",
    "bert_embeddings_matrix = np.array(bert_embeddings)\n",
    "print(bert_embeddings_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_unique_values_for_movie_property(movie_property):\n",
    "    cast_lists = list(filter(lambda item: item is not None, movie_details_df[movie_property]))\n",
    "    return {property for sublist in cast_lists for property in sublist}\n",
    "\n",
    "def get_OHE_columns_for_property(property):\n",
    "    unique_properties = get_unique_values_for_movie_property(property)\n",
    "    columns = [f\"{property_value}_{property}_OHE\" for property_value in unique_properties] #OH = one-hot encoding\n",
    "    return columns\n",
    "\n",
    "#Build an empty df of all imdb_movie_id, bert encodings and additional properties of the movie.\n",
    "def create_empty_movies_vector_df(bert_embeddings_matrix):\n",
    "    \n",
    "    movie_count, bert_dimensions = bert_embeddings_matrix.shape\n",
    "\n",
    "    genre_columns = get_OHE_columns_for_property(\"genres\")\n",
    "    cast_columns = get_OHE_columns_for_property(\"cast\")\n",
    "    director_columns = get_OHE_columns_for_property(\"director\")\n",
    "    bert_columns = [f'embed_{i}_OHE' for i in range(bert_dimensions)]\n",
    "    additional_columns = ['year_norm', 'runtimes_norm', 'rating_norm', 'votes_norm'] #these are already numerical values. They will be normalized to 0-1 range. \n",
    "\n",
    "    all_column_titles = ['imdb_movie_id', 'movie_title'] + bert_columns + cast_columns + director_columns + genre_columns + additional_columns\n",
    "    #df dimensions = movie_count x (768 bert_dimensions + unique_genre_count + unique_cast_count + unique_director_count + 1 for year + 1 for runtime + 1 rating + 1 for votes)\n",
    "    mega_df = pd.DataFrame(0, index = range(movie_count), columns = all_column_titles)\n",
    "    mega_df[\"imdb_movie_id\"] = movie_details_df['imdbID']\n",
    "    mega_df[\"movie_title\"] = movie_details_df['title']\n",
    "    return mega_df\n",
    "  \n",
    "mega_df = create_empty_movies_vector_df(bert_embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_OHE_for_movie_property(imdb_id, property, movie_details_df, mega_df):\n",
    "    movie_row = movie_details_df[movie_details_df['imdbID'] == imdb_id].iloc[0]\n",
    "    property_values = movie_row[property]\n",
    "    if property_values is not None:\n",
    "        for property_value in property_values:\n",
    "            property_col_name = f\"{property_value}_{property}_OHE\"\n",
    "            condition = mega_df['imdb_movie_id'] == imdb_id\n",
    "            mega_df.loc[condition, property_col_name] = 1\n",
    "\n",
    "def build_movies_vector_df(mega_df, bert_embeddings_matrix):\n",
    "\n",
    "    movie_count, bert_dimensions = bert_embeddings_matrix.shape\n",
    "    assert movie_count == len(mega_df), \"Row counts do not match.\"\n",
    "    assert bert_dimensions == 768, \"Embedding size is expected to be 768.\"\n",
    "\n",
    "    for movie in movie_details_df.itertuples():\n",
    "        imdb_id = movie.imdbID\n",
    "        movie_index = movie.Index\n",
    "        mega_df.iloc[movie_index, 2:2+bert_dimensions] = bert_embeddings_matrix[movie_index]\n",
    "\n",
    "        set_OHE_for_movie_property(imdb_id, \"genres\", movie_details_df, mega_df)\n",
    "        set_OHE_for_movie_property(imdb_id, \"cast\", movie_details_df, mega_df)\n",
    "        set_OHE_for_movie_property(imdb_id, \"director\", movie_details_df, mega_df)\n",
    "\n",
    "        condition = mega_df['imdb_movie_id'] == imdb_id\n",
    "        mega_df.loc[condition, \"year_norm\"] = movie.year\n",
    "        mega_df.loc[condition, \"runtimes_norm\"] = float(movie.runtimes[0]) if movie.runtimes is not None and len(movie.runtimes) > 0 else 0\n",
    "        mega_df.loc[condition, \"rating_norm\"] = movie.rating\n",
    "        mega_df.loc[condition, \"votes_norm\"] = movie.votes\n",
    "\n",
    "    #Apply Selective Normalization (min-max scaling for year, and standardization\n",
    "    minMaxScaler = MinMaxScaler()\n",
    "    robustScaler = RobustScaler()\n",
    "\n",
    "    for col in ['year_norm', 'runtimes_norm', 'rating_norm', 'votes_norm']:\n",
    "        # Apply RobustScaler\n",
    "        robust_scaled = robustScaler.fit_transform(mega_df[[col]])\n",
    "        # Apply MinMaxScaler to the output of RobustScaler\n",
    "        min_max_scaled = minMaxScaler.fit_transform(robust_scaled)\n",
    "        # Option 1: Replace original column\n",
    "        mega_df[col] = min_max_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes about 5 seconds to run for 284 movies x 2455 columns.\n",
    "build_movies_vector_df(mega_df, bert_embeddings_matrix)\n",
    "mega_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_indices_with_string(df, search_string):\n",
    "    return [i for i, key in enumerate(df.columns) if search_string in key]\n",
    "\n",
    "def get_weights(df):\n",
    "    # Find the indices\n",
    "    embedding_indices = find_indices_with_string(df, \"embed\")\n",
    "    genres_indices = find_indices_with_string(df, \"genres\")\n",
    "    cast_indices = find_indices_with_string(df, \"cast\")\n",
    "    director_indices = find_indices_with_string(df, \"director\")\n",
    "    year_norm_indices = find_indices_with_string(df, \"year_norm\")\n",
    "    rating_norm_indices = find_indices_with_string(df, \"rating_norm\")\n",
    "    votes_norm_indices = find_indices_with_string(df, \"votes_norm\")\n",
    "    remaining_indices = [num for num in range(len(df.columns)) if num not in set(embedding_indices + genres_indices + cast_indices + director_indices + year_norm_indices + rating_norm_indices + votes_norm_indices)]\n",
    "\n",
    "    # Weights allocation\n",
    "    embedding_total_weight = 80\n",
    "    genre_total_weight = 12\n",
    "    cast_weight = 5\n",
    "    director_weight = 5\n",
    "    rating_weight = 5\n",
    "    year_weight = 0\n",
    "    votes_weight = 1\n",
    "\n",
    "    # Initialize weights array with zeros\n",
    "    weights = [0] * len(df.columns)\n",
    "\n",
    "    # Function to distribute weights\n",
    "    def distribute_weights(indices, total_weight):\n",
    "        if indices:  # Avoid division by zero\n",
    "            per_feature_weight = total_weight / len(indices)\n",
    "            for index in indices:\n",
    "                weights[index] = per_feature_weight\n",
    "\n",
    "    # Distribute weights based on category\n",
    "    distribute_weights(embedding_indices, embedding_total_weight)\n",
    "    distribute_weights(genres_indices, genre_total_weight)\n",
    "    distribute_weights(cast_indices, cast_weight)\n",
    "    distribute_weights(director_indices, director_weight)\n",
    "    distribute_weights(year_norm_indices, year_weight)\n",
    "    distribute_weights(rating_norm_indices, rating_weight)\n",
    "    distribute_weights(votes_norm_indices, votes_weight)\n",
    "    distribute_weights(remaining_indices, 0)  # Assuming no additional weight for remaining features\n",
    "\n",
    "    weights = np.array(weights)\n",
    "    weights_series = pd.Series(weights, index=df.columns)\n",
    "    return weights_series\n",
    "\n",
    "def weighted_vectors(mega_df):\n",
    "    df = mega_df.drop(columns=['imdb_movie_id', 'movie_title'])\n",
    "    weights = get_weights(df)\n",
    "    weighted_df = df.mul(weights, axis=1)\n",
    "    \n",
    "    non_float_columns = mega_df[['imdb_movie_id', 'movie_title']]\n",
    "    weighted_df = pd.concat([non_float_columns, weighted_df], axis=1)\n",
    "\n",
    "    return weighted_df\n",
    "\n",
    "weighted_vectors(mega_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This calculates Cosines similarity between 2 vectors (movies).\n",
    "\n",
    "#Note: Cosine similarity expects 2D matrices.\n",
    "#To perform cosine similarity on vectors, remember to reshape the vector in the 2D shape (1, N), where N is the vector length.\n",
    "#to-do: Update this function to become a weighted cosine, using weights from a file.\n",
    "def get_cosine_similarity(movie_vector_1, movie_vector_2):\n",
    "    cosine_sim = cosine_similarity(movie_vector_1, movie_vector_2)\n",
    "    return cosine_sim\n",
    "\n",
    "#Get the top movies relating to a given movie vector using cosine similarity.\n",
    "#2 use cases for this:\n",
    "# 1. given_movie_vector = a specific movie's embeddings. This will return top movies relating to that movie.\n",
    "# 2. given_movie_vector = user_profile's vector. This will return top movies recommended for this user.\n",
    "\n",
    "def get_top_movies_cosine(tfidf_matrix, given_movie_vector, movie_titles, top_n=5):\n",
    "\n",
    "    # Compute cosine similarity between the movie at movie_index and all movies in the matrix\n",
    "    cosine_similarities = get_cosine_similarity(given_movie_vector, tfidf_matrix).flatten()\n",
    "\n",
    "    # Get the indices of the top_n movies with the highest cosine similarity scores\n",
    "    # Use argsort and reverse it with [::-1] to get the indices in descending order of similarity\n",
    "    # Skip the first one as it is the movie itself with a similarity of 1\n",
    "    similar_indices = cosine_similarities.argsort()[::-1][1:top_n+1]\n",
    "\n",
    "    # Get the scores for the top_n movies\n",
    "    similar_scores = cosine_similarities[similar_indices]\n",
    "\n",
    "    # Combine indices and scores into a list of tuples and return\n",
    "    top_movies = [(movie_titles[index], index, score) for index, score in zip(similar_indices, similar_scores)]\n",
    "\n",
    "    print(f\"Top similar movies to the provided movie vector:\\n\")\n",
    "    for num, (title, index, score) in enumerate(top_movies, start = 1):\n",
    "        print(f\"{num}. \\\"{title}\\\" at ROW {index} with similarity score: {score}\")\n",
    "\n",
    "    return top_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, title in enumerate(titles_with_synopsis):\n",
    "    print(i, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the movie you want to get recommendations for\n",
    "desired_row = mega_df[mega_df['movie_title'] == 'Oppenheimer'].index[0]\n",
    "# desired_row = 16\n",
    "\n",
    "print(\"Just BERT\")\n",
    "#Compare the above results with just Bert\n",
    "get_top_movies_cosine(bert_embeddings_matrix, bert_embeddings_matrix[desired_row].reshape(1, -1), titles_with_synopsis, 5)\n",
    "\n",
    "print(\"All properties\")\n",
    "mega_matrix = mega_df.drop(columns=['imdb_movie_id', 'movie_title']).values\n",
    "#Print top movies from cosine similarity on the mega DF\n",
    "get_top_movies_cosine(mega_matrix, mega_matrix[desired_row].reshape(1, -1), titles_with_synopsis, 5);\n",
    "\n",
    "print(\"All properties with weighted cosine\")\n",
    "weighted_mega_df = weighted_vectors(mega_df)\n",
    "weighted_mega_matrix = weighted_mega_df.drop(columns=['imdb_movie_id', 'movie_title']).values\n",
    "#Print top movies from cosine similarity on the mega DF\n",
    "get_top_movies_cosine(weighted_mega_matrix, weighted_mega_matrix[desired_row].reshape(1, -1), titles_with_synopsis, 5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate updated user profile after they have voted on M movies. \n",
    "# M = 1 means immediate feedback loop. But it may not be ideal. It might bias our recommendations towards our initial dataset (High exploit, low explore)\n",
    "# I think M = 5 or 10 might be better. \n",
    "# An even better idea is a hybrid of the above. M = 10 inititally, and after some votes M --> 1. \n",
    "\n",
    "def update_user_profile_batch(user_profile, unprocessed_movie_vectors, unprocessed_votes, processed_count):\n",
    "    \"\"\"\n",
    "    Update the user profile based on a batch of movie ratings.\n",
    "\n",
    "    :param user_profile: scipy.sparse matrix, the current user profile vector (1, N)\n",
    "    :param movie_vectors: list of scipy.sparse matrices, the TF-IDF vectors of the rated movies [(1, N), (1, N), ...]\n",
    "    :param ratings: list of str, the ratings for each movie ('like' or 'dislike')\n",
    "    :param processed_count: int, the number of movies that have already been processed to create the current user profile\n",
    "    :return: scipy.sparse matrix, the updated user profile vector (1, N)\n",
    "    \"\"\"\n",
    "    dislike_factor = 1/3 #we can tweak this to see impact on recommendations. \n",
    "\n",
    "    if len(unprocessed_movie_vectors) != len(unprocessed_votes):\n",
    "        raise ValueError(\"The number of movie vectors and ratings must be the same\")\n",
    "    \n",
    "    # Initialize a temporary profile change vector\n",
    "    profile_change = sparse.csr_matrix((1, user_profile.shape[1]))\n",
    "\n",
    "    # Process each movie vector and rating\n",
    "    for movie_vector, vote in zip(unprocessed_movie_vectors, unprocessed_votes):\n",
    "        if vote == 'like':\n",
    "            profile_change += movie_vector\n",
    "        elif vote == 'dislike':\n",
    "            profile_change -= (dislike_factor * movie_vector)\n",
    "        else:\n",
    "            raise ValueError(\"Rating must be 'like' or 'dislike'\")\n",
    "\n",
    "    # Update the user profile after processing M ratings\n",
    "    updated_profile = user_profile*processed_count + profile_change\n",
    "\n",
    "    # Normalize the updated profile\n",
    "    updated_profile = updated_profile/(processed_count + len(unprocessed_votes))\n",
    "\n",
    "    return updated_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#B. For testing user profile recommendations.\n",
    "#   Just list the movies and the votes. This code will return a new \"seeded\" user profile\n",
    "movie_titles = [\"Wonka\", \"Oppenheimer\", \"The Iron Claw\", \"Aquaman and the Lost Kingdom\", \"Family Switch\", \"Murder Mystery 2\", \"No Hard Feelings\", \"Insidious: The Red Door\", \"Shazam! Fury of the Gods\"]\n",
    "ratings = ['dislike', 'like', 'like', \"dislike\", \"like\", \"like\", \"dislike\", \"like\", \"dislike\"]\n",
    "\n",
    "movie_titles = [\"The Super Mario Bros. Movie\", \"The Marvels\", \"Oppenheimer\", \"Beau Is Afraid\", \"Spider-Man: Across the Spider-Verse\"]\n",
    "ratings = ['like', 'like', 'dislike', 'dislike', 'like']\n",
    "\n",
    "#Get the selected rows as a dataframe\n",
    "selected_rows = pd.DataFrame()\n",
    "for title in movie_titles:\n",
    "    selected_rows = pd.concat([selected_rows, weighted_mega_df[weighted_mega_df['movie_title'] == title]], ignore_index=True)\n",
    "\n",
    "#Take the selected movies df, and convert to list of sparse matrices (1, vector length), but remove the first two columns which are IMDB id and title.\n",
    "selected_movie_vectors = [sparse.csr_matrix(row.reshape(1, -1)) for row in selected_rows.iloc[:, 2:].values]\n",
    "\n",
    "#initialize empty user_profile of the right shape.\n",
    "user_profile = sparse.csr_matrix((1, selected_movie_vectors[0].shape[1]), dtype= float)\n",
    "\n",
    "updated_user_profile = update_user_profile_batch(user_profile, selected_movie_vectors, ratings, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the top movies recommended for the above generated user profile\n",
    "get_top_movies_cosine(weighted_mega_matrix, updated_user_profile, titles_with_synopsis, 10);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For future convenience, we should have an easy way to go from IMDB ID -> title and vice versa:\n",
    "\n",
    "id_to_title_dict = pd.Series(weighted_mega_df.movie_title.values,index=weighted_mega_df.imdb_movie_id).to_dict()\n",
    "title_to_id_dict = pd.Series(weighted_mega_df.imdb_movie_id.values,index=weighted_mega_df.movie_title).to_dict()\n",
    "\n",
    "movie_title = 'Wonka'  # Example movie title\n",
    "movie_id = title_to_id_dict.get(movie_title)\n",
    "\n",
    "print(f\"The IMDb ID for '{movie_title}' is {movie_id}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables:\n",
    "\n",
    "#1. Room profile: the aggregated profile of the full room.\n",
    "room_profile = sparse.csr_matrix((1, selected_movie_vectors[0].shape[1]), dtype= float)\n",
    "\n",
    "#2. All user profiles as a dictionary, indexed by user_id\n",
    "user_profiles = {}\n",
    "\n",
    "#3. History of all movies shown to all users:\n",
    "shown_movies_history = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. All user's CURRENT voting activity, indexed by user_id, then by processed/unprocessed and then by movie_id\n",
    "# Note that this is different from \"all_user_votes\" further below which is coming from the csv directly. \n",
    "# Effectively that variable can see the future. This variable here is the CURRENT voting activity.\n",
    "# structure will be: UserId: <list of imdb movie ids>\n",
    "user_votes_activity = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def read_user_votes(filename):\n",
    "    user_votes = []\n",
    "    try:\n",
    "        with open(filename, newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                # Convert row to dictionary and append to the list\n",
    "                user_votes.append({'UserId': row['UserId'], 'Movie': row['Movie'], 'Vote': row['Vote']})\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {filename} was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return user_votes\n",
    "\n",
    "filename = 'userVotes.csv'\n",
    "all_user_votes = read_user_votes(filename)\n",
    "print(all_user_votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_user_votes(user_id, movie, vote, user_votes_activity, title_to_id_dict):\n",
    "    \n",
    "    if user_id not in user_votes_activity:\n",
    "        user_votes_activity[user_id] = {\n",
    "            'processed_movies': [],\n",
    "            'processed_votes': [],\n",
    "            'unprocessed_movies': [],\n",
    "            'unprocessed_votes': [],\n",
    "        }\n",
    "    \n",
    "    #Now the user_id key will definitely exist in the dictionary. Now add the new vote.\n",
    "    \n",
    "    #As a principle, let's save movie_id in our internal variables. The csv can be the title for ease of testing.\n",
    "    #Using strip() to remove any whitespace at the ends which can cause the movie_id to not be found\n",
    "        \n",
    "    movie_id = title_to_id_dict.get(movie.strip()) #imdb ID\n",
    "\n",
    "    user_votes_activity[user_id]['unprocessed_movies'].append(movie_id)\n",
    "    user_votes_activity[user_id]['unprocessed_votes'].append(vote.strip())\n",
    "\n",
    "    print(\"Update vote history for this user:\", user_votes_activity[user_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_user_profile(user_id, movie, vote, user_votes_activity, user_profiles, weighted_mega_df):\n",
    "\n",
    "    MIN_VOTES_BEFORE_UPDATE = 5\n",
    "\n",
    "    unprocessed_movie_ids = user_votes_activity[user_id]['unprocessed_movies']\n",
    "    unprocessed_votes = user_votes_activity[user_id]['unprocessed_votes']\n",
    "    processed_movie_ids = user_votes_activity[user_id]['processed_movies']\n",
    "    processed_votes = user_votes_activity[user_id]['processed_votes']\n",
    "\n",
    "    if len(unprocessed_movie_ids) >= MIN_VOTES_BEFORE_UPDATE:\n",
    "        #Get the selected rows as a dataframe\n",
    "        unprocessed_rows = pd.DataFrame()\n",
    "\n",
    "        for movie_id in unprocessed_movie_ids:\n",
    "            unprocessed_rows = pd.concat([unprocessed_rows, weighted_mega_df[weighted_mega_df['imdb_movie_id'] == movie_id]], ignore_index=True)\n",
    "\n",
    "        #Take the selected movies df, and convert to list of sparse matrices (1, vector length), but remove the first two columns which are IMDB id and title.\n",
    "        unprocessed_movie_vectors = [sparse.csr_matrix(row.reshape(1, -1)) for row in unprocessed_rows.iloc[:, 2:].values]\n",
    "\n",
    "        #initialize empty user_profile of the right shape.\n",
    "        if user_id not in user_profiles:\n",
    "            current_user_profile = sparse.csr_matrix((1, unprocessed_movie_vectors[0].shape[1]), dtype= float)\n",
    "        else:\n",
    "            current_user_profile = user_profiles[user_id]\n",
    "\n",
    "        #Calculate the updated profile:\n",
    "        updated_user_profile = update_user_profile_batch(current_user_profile, unprocessed_movie_vectors, unprocessed_votes, len(processed_votes))\n",
    "        user_profiles[user_id] = updated_user_profile\n",
    "\n",
    "        #Move the unprocessed movies and IDs into the processed lists, and empty the unprocessed lists. \n",
    "        user_votes_activity[user_id]['processed_movies'].extend(user_votes_activity[user_id]['unprocessed_movies'])\n",
    "        user_votes_activity[user_id]['processed_votes'].extend(user_votes_activity[user_id]['unprocessed_votes'])\n",
    "\n",
    "        user_votes_activity[user_id]['unprocessed_movies'] = []\n",
    "        user_votes_activity[user_id]['unprocessed_votes'] = []\n",
    "\n",
    "        print(\"Updated user profile for the user: \", user_id)\n",
    "\n",
    "    else:\n",
    "        print(\"Did NOT update user profile for user\", user_id, \"because not yet reached MIN vote count\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accept_user_vote(user_vote, user_votes_activity):\n",
    "    user_id = user_vote[\"UserId\"]\n",
    "    movie = user_vote[\"Movie\"]\n",
    "    vote = user_vote[\"Vote\"]\n",
    "    \n",
    "    # Every time a user votes, save the user vote\n",
    "    save_user_votes(user_id, movie, vote, user_votes_activity, title_to_id_dict)\n",
    "\n",
    "    update_user_profile(user_id, movie, vote, user_votes_activity, user_profiles, weighted_mega_df)\n",
    "    \n",
    "    #placeholder for now\n",
    "    update_room_profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After csv stuff, test execution starts here:\n",
    "\n",
    "#For convenience, you MIGHT want to reset user_votes_activity if you want to start testing from a clean slate:\n",
    "# user_votes_activity = {}\n",
    "\n",
    "for user_vote in all_user_votes:\n",
    "    accept_user_vote(user_vote, user_votes_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For testing the recommendations directly for a specific user profile\n",
    "#Remember, the user profile only populates after 5 initial (seed) votes. \n",
    "#Make sure 5 movies are there for this user in the csv otherwise there won't be any profile for this user.\n",
    "user_profile = user_profiles['2']\n",
    "\n",
    "updated_user_profile = update_user_profile_batch(user_profile, selected_movie_vectors, ratings, 0)\n",
    "\n",
    "get_top_movies_cosine(weighted_mega_matrix, updated_user_profile, titles_with_synopsis, 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_room_profile():\n",
    "    \n",
    "    room_profile = False\n",
    "    #print(\"Dummy: Updated room profile\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
