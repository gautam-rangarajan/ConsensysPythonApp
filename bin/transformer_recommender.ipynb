{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, normalize\n",
    "from scipy import sparse\n",
    "import sys\n",
    "\n",
    "# Get the current working directory of the Jupyter notebook\n",
    "notebook_directory = os.getcwd()\n",
    "# Assuming the notebook is in the 'bin/' folder, add the parent directory to sys.path\n",
    "parent_directory = os.path.dirname(notebook_directory)\n",
    "sys.path.append(parent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "cache_file = Path(\"movie_synopsis_cache.json\")\n",
    "\n",
    "# Function to load cache data from a file\n",
    "def load_cache():\n",
    "    if cache_file.is_file() and cache_file.stat().st_size > 0:\n",
    "        with open(cache_file, 'r') as file:\n",
    "            try:\n",
    "                return json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                return {}\n",
    "    return {}\n",
    "\n",
    "# Function to save cache data to a file\n",
    "def save_cache(cache):\n",
    "    with open(cache_file, 'w') as file:\n",
    "        json.dump(cache, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from imdb import Cinemagoer\n",
    "# Create an instance of the Cinemagoer class\n",
    "cg = Cinemagoer()\n",
    "\n",
    "# Create methods to fetch movie details given a list of imdb movie ids\n",
    "def get_movie_details(imdb_id):\n",
    "    cache = load_cache()\n",
    "\n",
    "    # Check if the movie data is in cache\n",
    "    if imdb_id in cache:\n",
    "        print(\"Retrieved from cache.\")\n",
    "        return cache[imdb_id]\n",
    "\n",
    "    # If not in cache, get movie data\n",
    "    start = timeit.default_timer()\n",
    "    cg_imdb_id = imdb_id.replace(\"tt\", \"\")\n",
    "    movie = cg.get_movie(cg_imdb_id)\n",
    "    end = timeit.default_timer()\n",
    "    print(\"get_movie_details took {} seconds to run\".format(end - start))\n",
    "    result = {}\n",
    "\n",
    "    keys = [\"title\", \"genres\", \"runtimes\", \"original air date\", \"rating\", \"votes\", \"imdbID\", \"language codes\", \"year\", \"director\", \"cast\"]\n",
    "    for key in keys:\n",
    "        if key not in movie:\n",
    "            result[key] = None\n",
    "        elif key == \"cast\":\n",
    "            result[key] = [c.personID for c in movie[key][:5]]\n",
    "        elif key == \"director\":\n",
    "            result[key] = [c.personID for c in movie[key]]\n",
    "        else:\n",
    "            result[key] = movie.get(key, None)\n",
    "\n",
    "    synopsis_present = True if \"synopsis\" in movie and len(movie[\"synopsis\"]) > 0 else False\n",
    "    plot_present = True if \"plot\" in movie and len(movie[\"plot\"]) > 0 else False\n",
    "    if synopsis_present and plot_present:\n",
    "        result[\"synopsis\"] = movie[\"synopsis\"][0]\n",
    "        result[\"plot\"] = movie[\"plot\"][0]\n",
    "    elif synopsis_present:\n",
    "        result[\"synopsis\"] = movie[\"synopsis\"][0]\n",
    "        result[\"plot\"] = movie[\"synopsis\"][0]\n",
    "    elif plot_present:\n",
    "        result[\"synopsis\"] = movie[\"plot\"][0]\n",
    "        result[\"plot\"] = movie[\"plot\"][0]\n",
    "    else:\n",
    "        result[\"synopsis\"] = \"\"\n",
    "        result[\"plot\"] = \"\"\n",
    "\n",
    "    # Save the new data to cache\n",
    "    print(\"trying to save \", imdb_id)\n",
    "    cache[imdb_id] = result\n",
    "    save_cache(cache)\n",
    "    return result\n",
    "\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(1))  # Retries up to 3 times with a 1-second wait between tries\n",
    "def get_movie_details_with_retry(movie):\n",
    "    return get_movie_details(movie)\n",
    "\n",
    "def get_movie_details_as_data_frame(movie_list):\n",
    "    all_movie_details = {}\n",
    "    for movie in movie_list:\n",
    "        all_movie_details[movie] = get_movie_details_with_retry(movie)\n",
    "    all_movie_details = [all_movie_details[movie] for movie in movie_list if movie in all_movie_details]\n",
    "    return pd.json_normalize(all_movie_details)\n",
    "\n",
    "print(get_movie_details_as_data_frame([\"tt6166392\", \"tt4046784\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Given a date range, fetch all the movies that were released during that period.\n",
    "# Additional filters like language/minimum vote count can also be specified\n",
    "MINIMUM_VOTE_COUNT = 50\n",
    "LANGUAGES = [\"en\"]\n",
    "\n",
    "def get_tmdb_movies_in_range(start, end):\n",
    "    api_key = '0b2cc6b5655e6c00206bd71118d1156f'\n",
    "    languages = \",\".join(LANGUAGES)\n",
    "    url = f'https://api.themoviedb.org/3/discover/movie?api_key={api_key}&primary_release_date.gte={start}&primary_release_date.lte={end}&include_adult=false&include_video=false&with_original_language={languages}&page=1&sort_by=popularity.desc&vote_count.gte={MINIMUM_VOTE_COUNT}'\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    total_pages = data[\"total_pages\"]\n",
    "    total_results = data[\"total_results\"]\n",
    "    movies_in_date_range = []\n",
    "    print(f\"total_results: {total_results}\")\n",
    "\n",
    "    for page in range(total_pages):\n",
    "        try:\n",
    "            url = f'https://api.themoviedb.org/3/discover/movie?api_key={api_key}&primary_release_date.gte={start}&primary_release_date.lte={end}&include_adult=false&include_video=false&with_original_language={languages}&page={page+1}&sort_by=popularity.desc'\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "            movies_in_date_range.extend(data[\"results\"])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(.1)\n",
    "    print(f\"total_results extracted: {len(movies_in_date_range)}\")\n",
    "    return movies_in_date_range\n",
    "\n",
    "def get_imdb_ids_for_tmdb_movies_in_range(start, end):\n",
    "    api_key = '0b2cc6b5655e6c00206bd71118d1156f'\n",
    "\n",
    "    movies = get_tmdb_movies_in_range(start, end)\n",
    "    imdb_ids = []\n",
    "    found_movies = []\n",
    "    low_votes_movies = []\n",
    "    missing_movies = []\n",
    "    for movie in movies:\n",
    "        try:\n",
    "            id = movie[\"id\"]\n",
    "            url = f\"https://api.themoviedb.org/3/movie/{id}/external_ids?api_key={api_key}\"\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "            imdb_id = data[\"imdb_id\"]\n",
    "            if imdb_id is not None:\n",
    "                if int(movie[\"vote_count\"]) >= MINIMUM_VOTE_COUNT:\n",
    "                    imdb_ids.append(imdb_id)\n",
    "                    found_movies.append((id, movie[\"original_title\"], movie[\"vote_count\"]))\n",
    "                else:\n",
    "                    low_votes_movies.append((id, movie[\"original_title\"], movie[\"vote_count\"]))\n",
    "            else:\n",
    "                missing_movies.append((id, movie[\"original_title\"], movie[\"vote_count\"]))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(1)\n",
    "    print(f\"Number of imdb ids extracted: {len(imdb_ids)}\")\n",
    "    print(f\"Missing movies: {missing_movies}\")\n",
    "    print(f\"Low votes movies: {low_votes_movies}\")\n",
    "    print(f\"Found movies: {found_movies}\")\n",
    "    return imdb_ids\n",
    "\n",
    "yesterday = (datetime.now() - timedelta(1)).strftime('%Y-%m-%d') # Eventually, we will use this in the cron job that runs to populate for the last 'n' days\n",
    "start = \"2023-01-01\"\n",
    "end = \"2023-12-31\"\n",
    "imdb_movie_ids = get_imdb_ids_for_tmdb_movies_in_range(start, end)\n",
    "\n",
    "print(imdb_movie_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_movie_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each movie discovered, fetch full details using Cinemagoer\n",
    "movie_details_df = get_movie_details_as_data_frame(imdb_movie_ids)\n",
    "titles_with_synopsis = movie_details_df['title'].tolist()\n",
    "synopsis_list = movie_details_df['synopsis'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(synopsis_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "\n",
    "# Initialize tokenizer and model from pre-trained BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to create embeddings for a list of synopses using BERT\n",
    "def get_bert_embeddings(synopses):\n",
    "    embeddings = []\n",
    "    for synopsis in synopses:\n",
    "        # Tokenize the synopsis and convert to input format expected by BERT\n",
    "        inputs = tokenizer(synopsis, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        # Get the output from BERT model\n",
    "        outputs = model(**inputs)\n",
    "        # Use the mean of the last hidden state as the embedding\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "# Create BERT embeddings for the synopses\n",
    "bert_embeddings = get_bert_embeddings(synopsis_list)\n",
    "bert_embeddings_matrix = np.array(bert_embeddings)\n",
    "print(bert_embeddings_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bert_embeddings_matrix.shape)\n",
    "print(bert_embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This calculates Cosines similarity between 2 vectors (movies).\n",
    "\n",
    "#Note: Cosine similarity expects 2D matrices.\n",
    "#To perform cosine similarity on vectors, remember to reshape the vector in the 2D shape (1, N), where N is the vector length.\n",
    "#to-do: Update this function to become a weighted cosine, using weights from a file.\n",
    "def get_cosine_similarity(movie_vector_1, movie_vector_2):\n",
    "    cosine_sim = cosine_similarity(movie_vector_1, movie_vector_2)\n",
    "    return cosine_sim\n",
    "\n",
    "#Get the top movies relating to a given movie vector using cosine similarity.\n",
    "#2 use cases for this:\n",
    "# 1. given_movie_vector = a specific movie's embeddings. This will return top movies relating to that movie.\n",
    "# 2. given_movie_vector = user_profile's vector. This will return top movies recommended for this user.\n",
    "\n",
    "def get_top_movies_cosine(tfidf_matrix, given_movie_vector, movie_titles, top_n=5):\n",
    "\n",
    "    # Compute cosine similarity between the movie at movie_index and all movies in the matrix\n",
    "    cosine_similarities = get_cosine_similarity(given_movie_vector, tfidf_matrix).flatten()\n",
    "\n",
    "    # Get the indices of the top_n movies with the highest cosine similarity scores\n",
    "    # Use argsort and reverse it with [::-1] to get the indices in descending order of similarity\n",
    "    # Skip the first one as it is the movie itself with a similarity of 1\n",
    "    similar_indices = cosine_similarities.argsort()[::-1][1:top_n+1]\n",
    "\n",
    "    # Get the scores for the top_n movies\n",
    "    similar_scores = cosine_similarities[similar_indices]\n",
    "\n",
    "    # Combine indices and scores into a list of tuples and return\n",
    "    top_movies = [(movie_titles[index], index, score) for index, score in zip(similar_indices, similar_scores)]\n",
    "\n",
    "    print(f\"Top similar movies to the provided movie vector:\\n\")\n",
    "    for num, (title, index, score) in enumerate(top_movies, start = 1):\n",
    "        print(f\"{num}. \\\"{title}\\\" at ROW {index} with similarity score: {score}\")\n",
    "\n",
    "    return top_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, title in enumerate(titles_with_synopsis):\n",
    "    print(i, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_values_from_cache(movie_property):\n",
    "\n",
    "    cache = load_cache()\n",
    "\n",
    "    unique_values = set()\n",
    "    # Iterate over each movie and update the set with genres from each movie\n",
    "    for movie_id, movie_info in cache.items():\n",
    "        genres = movie_info.get(movie_property, [])\n",
    "        unique_values.update(genres)    \n",
    "    return unique_values\n",
    "\n",
    "print(get_unique_values_from_cache(\"genres\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Build an empty df of all imdb_movie_id, bert encodings and additional properties of the movie.\n",
    "\n",
    "def create_empty_movies_vector_df(bert_embeddings_matrix):\n",
    "    \n",
    "    movie_count, bert_dimensions = bert_embeddings_matrix.shape\n",
    "\n",
    "    unique_genres = get_unique_values_from_cache(\"genres\")\n",
    "    genre_columns = [f\"{genre}_OHE\" for genre in unique_genres] #OH = one-hot encoding\n",
    "\n",
    "    unique_cast = get_unique_values_from_cache(\"cast\")\n",
    "    cast_columns = [f\"{cast}_OHE\" for cast in unique_cast] #OH = one-hot encoding\n",
    "\n",
    "    unique_directors = get_unique_values_from_cache(\"director\")\n",
    "    director_columns = [f\"{director}_OHE\" for director in unique_directors] #OH = one-hot encoding\n",
    "\n",
    "    bert_column_names = [f'embed_{i}' for i in range(bert_dimensions)]\n",
    "    additional_columns = ['year_norm', 'runtimes_norm', 'rating_norm', 'votes_norm'] #these are already numerical values. They will be normalized to 0-1 range. \n",
    "\n",
    "    all_column_titles = ['imdb_movie_id', 'movie_title'] + bert_column_names + genre_columns + cast_columns + director_columns + additional_columns\n",
    "    #df dimensions = movie_count x (768 bert_dimensions + unique_genre_count + unique_cast_count + unique_director_count + 1 for year + 1 for runtime + 1 rating + 1 for votes)\n",
    "    mega_df = pd.DataFrame(0, index = range(movie_count), columns = all_column_titles)\n",
    "\n",
    "    return mega_df\n",
    "\n",
    "mega_df = create_empty_movies_vector_df(bert_embeddings_matrix)\n",
    "mega_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_movies_vector_df(mega_df, bert_embeddings_matrix, cache, imdb_movie_ids):\n",
    "\n",
    "    movie_count, bert_dimensions = bert_embeddings_matrix.shape\n",
    "    assert movie_count == len(mega_df), \"Row counts do not match.\"\n",
    "    assert bert_dimensions == 768, \"Embedding size is expected to be 768.\"\n",
    "\n",
    "    for i, row in mega_df.iterrows():\n",
    "\n",
    "        movie = cache[imdb_movie_ids[i]]\n",
    "\n",
    "        mega_df.at[i,'imdb_movie_id'] = imdb_movie_ids[i]\n",
    "        mega_df.at[i,'movie_title'] = movie['title']\n",
    "        mega_df.iloc[i, 2:770] = bert_embeddings_matrix[i]\n",
    "        \n",
    "        #Handle all one-hot encoding. Remember that the mega_df was already initialized with 0s, so I just need to set the 1s.\n",
    "        genres = movie['genres']\n",
    "        for genre in genres:\n",
    "            genre_col_name = f\"{genre}_OHE\"\n",
    "            # Set the corresponding genre column to 1\n",
    "            if genre_col_name in mega_df.columns:\n",
    "                mega_df.at[i, genre_col_name] = 1\n",
    "        \n",
    "        cast = movie['cast']\n",
    "        for cast_member in cast:\n",
    "            cast_col_name = f\"{cast_member}_OHE\"\n",
    "            # Set the corresponding genre column to 1\n",
    "            if cast_col_name in mega_df.columns:\n",
    "                mega_df.at[i, cast_col_name] = 1\n",
    "\n",
    "        directors = movie['director']\n",
    "        for director in directors:\n",
    "            director_col_name = f\"{director}_OHE\"\n",
    "            # Set the corresponding genre column to 1\n",
    "            if director_col_name in mega_df.columns:\n",
    "                mega_df.at[i, director_col_name] = 1\n",
    "\n",
    "        mega_df.at[i, 'year_norm'] = movie['year']\n",
    "        mega_df.at[i, 'runtimes_norm'] = float(movie['runtimes'][0]) #damn random, but this field is a list of strings in our cache (and hence from TMDB). Need to make it a number.\n",
    "        mega_df.at[i, 'rating_norm'] = movie['rating']\n",
    "        mega_df.at[i, 'votes_norm'] = movie['votes']\n",
    "\n",
    "\n",
    "    #Apply Selective Normalization (min-max scaling for year, and standardization\n",
    "    minMaxScaler = MinMaxScaler()\n",
    "    standardScaler = StandardScaler()\n",
    "    robustScaler = RobustScaler()\n",
    "\n",
    "    for col in ['year_norm', 'runtimes_norm', 'rating_norm', 'votes_norm']:\n",
    "    \n",
    "        # Apply RobustScaler\n",
    "        robust_scaled = robustScaler.fit_transform(mega_df[[col]])\n",
    "        \n",
    "        # Apply MinMaxScaler to the output of RobustScaler\n",
    "        min_max_scaled = minMaxScaler.fit_transform(robust_scaled)\n",
    "        \n",
    "        # Option 1: Replace original column\n",
    "        mega_df[col] = min_max_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes about 5 seconds to run for 284 movies x 2455 columns.\n",
    "cache = load_cache()\n",
    "build_movies_vector_df(mega_df, bert_embeddings_matrix, cache, imdb_movie_ids)\n",
    "mega_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Had to run this for a few movies because they were not in the cache. Don't know why, maybe it didn't go through originally even with 3 retries.\n",
    "get_movie_details_as_data_frame([\"tt15210256\", \"tt7798604\", \"tt4046784\", \"tt28912858\", \"tt15351980\", \"tt15351980\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A. for testing recommendations against a specific movie.\n",
    "mega_matrix = sparse.csr_matrix(mega_df.drop(columns=['imdb_movie_id', 'movie_title']).values)\n",
    "\n",
    "#Specify the movie you want to get recommendations for here:\n",
    "desired_row = mega_df[mega_df['movie_title'] == 'Aquaman and the Lost Kingdom'].index[0]\n",
    "\n",
    "#Print top movies from cosine similarity on the mega DF\n",
    "get_top_movies_cosine(mega_matrix, mega_matrix[desired_row].reshape(1, -1), titles_with_synopsis, 5);\n",
    "print('\\nBERT alone:')\n",
    "\n",
    "#Compare the above results with just Bert\n",
    "get_top_movies_cosine(bert_embeddings_matrix, bert_embeddings_matrix[desired_row].reshape(1, -1), titles_with_synopsis, 5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate updated user profile after they have voted on M movies. \n",
    "# M = 1 means immediate feedback loop. But it may not be ideal. It might bias our recommendations towards our initial dataset (High exploit, low explore)\n",
    "# I think M = 5 or 10 might be better. \n",
    "# An even better idea is a hybrid of the above. M = 10 inititally, and after some votes M --> 1. \n",
    "\n",
    "def update_user_profile_batch(user_profile, movie_vectors, ratings, M):\n",
    "    \"\"\"\n",
    "    Update the user profile based on a batch of movie ratings.\n",
    "\n",
    "    :param user_profile: scipy.sparse matrix, the current user profile vector (1, N)\n",
    "    :param movie_vectors: list of scipy.sparse matrices, the TF-IDF vectors of the rated movies [(1, N), (1, N), ...]\n",
    "    :param ratings: list of str, the ratings for each movie ('like' or 'dislike')\n",
    "    :param M: int, the number of ratings to process before updating the profile\n",
    "    :return: scipy.sparse matrix, the updated user profile vector (1, N)\n",
    "    \"\"\"\n",
    "    dislike_factor = 1/3 #we can tweak this to see impact on recommendations. \n",
    "\n",
    "    if len(movie_vectors) != len(ratings):\n",
    "        raise ValueError(\"The number of movie vectors and ratings must be the same\")\n",
    "\n",
    "    if len(movie_vectors) < M:\n",
    "        raise ValueError(\"The number of movie vectors must be at least M\")\n",
    "\n",
    "    # Initialize a temporary profile change vector\n",
    "    profile_change = sparse.csr_matrix((1, user_profile.shape[1]))\n",
    "\n",
    "    # Process each movie vector and rating\n",
    "    for movie_vector, rating in zip(movie_vectors, ratings):\n",
    "        if rating == 'like':\n",
    "            profile_change += movie_vector\n",
    "        elif rating == 'dislike':\n",
    "            profile_change -= (dislike_factor * movie_vector)\n",
    "        else:\n",
    "            raise ValueError(\"Rating must be 'like' or 'dislike'\")\n",
    "\n",
    "    # Update the user profile after processing M ratings\n",
    "    updated_profile = user_profile + profile_change\n",
    "\n",
    "    # Normalize the updated profile\n",
    "    updated_profile = normalize(updated_profile, norm='l2', axis=1)\n",
    "\n",
    "    return updated_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B. For testing user profile recommendations.\n",
    "#   Just list the movies and the votes. This code will return a new \"seeded\" user profile\n",
    "movie_titles = [\"Barbie\", \"Ant-Man and the Wasp: Quantumania\", \"The Marvels\", \"Aquaman and the Lost Kingdom\"]\n",
    "ratings = ['dislike', 'like', 'like', \"like\"]\n",
    "\n",
    "#Get the selected rows as a dataframe\n",
    "selected_rows = pd.DataFrame()\n",
    "for title in movie_titles:\n",
    "    selected_rows = pd.concat([selected_rows, mega_df[mega_df['movie_title'] == title]], ignore_index=True)\n",
    "\n",
    "#Take the selected movies df, and convert to list of sparse matrices (1, vector length), but remove the first two columns which are IMDB id and title.\n",
    "selected_movie_vectors = [sparse.csr_matrix(row.reshape(1, -1)) for row in selected_rows.iloc[:, 2:].values]\n",
    "\n",
    "#initialize empty user_profile of the right shape.\n",
    "user_profile = sparse.csr_matrix((1, selected_movie_vectors[0].shape[1]), dtype= float)\n",
    "\n",
    "updated_user_profile = update_user_profile_batch(user_profile, selected_movie_vectors, ratings, len(ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the top movies recommended for the above generated user profile\n",
    "get_top_movies_cosine(mega_matrix, updated_user_profile, titles_with_synopsis, 5);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
