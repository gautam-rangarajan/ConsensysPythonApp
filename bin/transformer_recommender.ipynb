{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import sys\n",
    "\n",
    "# Get the current working directory of the Jupyter notebook\n",
    "notebook_directory = os.getcwd()\n",
    "# Assuming the notebook is in the 'bin/' folder, add the parent directory to sys.path\n",
    "parent_directory = os.path.dirname(notebook_directory)\n",
    "sys.path.append(parent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from imdb import Cinemagoer\n",
    "import timeit\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "cache_file = Path(\"movie_synopsis_cache.json\")\n",
    "# Create an instance of the Cinemagoer class\n",
    "cg = Cinemagoer()\n",
    "\n",
    "# Function to load cache data from a file\n",
    "def load_cache():\n",
    "    if cache_file.is_file() and cache_file.stat().st_size > 0:\n",
    "        with open(cache_file, 'r') as file:\n",
    "            try:\n",
    "                return json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                return {}\n",
    "    return {}\n",
    "\n",
    "# Function to save cache data to a file\n",
    "def save_cache(cache):\n",
    "    with open(cache_file, 'w') as file:\n",
    "        json.dump(cache, file, indent=4)\n",
    "\n",
    "def get_movie_description(imdb_id):\n",
    "    cache = load_cache()\n",
    "\n",
    "    # Check if the movie data is in cache\n",
    "    if imdb_id in cache:\n",
    "        print(\"Retrieved from cache.\")\n",
    "        return cache[imdb_id]\n",
    "\n",
    "    # If not in cache, get movie data\n",
    "    start = timeit.default_timer()\n",
    "    cg_imdb_id = imdb_id.replace(\"tt\", \"\")\n",
    "    movie = cg.get_movie(cg_imdb_id)\n",
    "    end = timeit.default_timer()\n",
    "    print(\"get_movie_description took {} seconds to run\".format(end - start))\n",
    "    result = {}\n",
    "    for info in movie.current_info:\n",
    "        if info in movie:\n",
    "            result[info] = movie[info]\n",
    "\n",
    "    # Save the new data to cache\n",
    "    cache[imdb_id] = result\n",
    "    save_cache(cache)\n",
    "    return result\n",
    "\n",
    "imdb_id = 'tt3469046'  # Example IMDb ID 'tt0111161' for \"The Shawshank Redemption\"\n",
    "print(get_movie_description(imdb_id))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filter movies from movies_metadata.csv\n",
    "def contains_english(lang_list):\n",
    "    try:\n",
    "        return \"[{'iso_639_1': 'en', 'name': 'English'}]\" in lang_list\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Reading from the movies_metadata.csv file\n",
    "# Required fields:\n",
    "# 1. imdb_id\n",
    "# 2. release_date\n",
    "# 3. spoken_languages\n",
    "# 4. vote_count\n",
    "def load_english_movies(years=\"\", min_reviews=0):\n",
    "    year_filter = \"|\".join(years)\n",
    "    df_extended = pd.read_csv(\"../movies_metadata.csv\")\n",
    "    filtered_df_extended = df_extended[(df_extended['release_date'].str.contains(year_filter, na=False)) & df_extended['spoken_languages'].apply(contains_english)]\n",
    "    return filtered_df_extended[filtered_df_extended['vote_count'] >= min_reviews]\n",
    "\n",
    "filtered_df_extended = load_english_movies([\"2015\", \"2016\", \"2017\"], 500)\n",
    "imdb_movie_ids = filtered_df_extended['imdb_id'].tolist()\n",
    "print(imdb_movie_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_synopsis_for_movie(imdb_movie_id):\n",
    "    try:\n",
    "        movie_description = get_movie_description(imdb_movie_id)\n",
    "        movie_synopsis = \"\"\n",
    "        if \"synopsis\" in movie_description and len(movie_description[\"synopsis\"]) > 0:\n",
    "            movie_synopsis = movie_description[\"synopsis\"][0]\n",
    "        elif \"plot\" in movie_description and len(movie_description[\"plot\"]) > 0:\n",
    "            movie_synopsis = movie_description[\"plot\"][0]\n",
    "        return movie_synopsis\n",
    "    except:\n",
    "        print(f\"Movie with id {imdb_movie_id} ran into an error! Skipping...\")\n",
    "        return None\n",
    "\n",
    "filtered_df_extended['cleaned_synopsis'] = filtered_df_extended['imdb_id'].apply(get_synopsis_for_movie)\n",
    "synopsis_list = filtered_df_extended['cleaned_synopsis'].tolist()\n",
    "titles_with_synopsis = filtered_df_extended['title'].tolist()\n",
    "print(filtered_df_extended['cleaned_synopsis'].sample(n=2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "\n",
    "# Initialize tokenizer and model from pre-trained BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to create embeddings for a list of synopses using BERT\n",
    "def get_bert_embeddings(synopses):\n",
    "    embeddings = []\n",
    "    for synopsis in synopses:\n",
    "        # Tokenize the synopsis and convert to input format expected by BERT\n",
    "        inputs = tokenizer(synopsis, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        # Get the output from BERT model\n",
    "        outputs = model(**inputs)\n",
    "        # Use the mean of the last hidden state as the embedding\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "# Create BERT embeddings for the synopses\n",
    "bert_embeddings = get_bert_embeddings(synopsis_list)\n",
    "bert_embeddings_matrix = np.array(bert_embeddings)\n",
    "print(bert_embeddings_matrix.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#This calculates Cosines similarity between 2 vectors (movies).\n",
    "\n",
    "#Note: Cosine similarity expects 2D matrices.\n",
    "#To perform cosine similarity on vectors, remember to reshape the vector in the 2D shape (1, N), where N is the vector length.\n",
    "#to-do: Update this function to become a weighted cosine, using weights from a file.\n",
    "def get_cosine_similarity(movie_vector_1, movie_vector_2):\n",
    "    cosine_sim = cosine_similarity(movie_vector_1, movie_vector_2)\n",
    "    return cosine_sim\n",
    "\n",
    "#Get the top movies relating to a given movie vector using cosine similarity.\n",
    "#2 use cases for this:\n",
    "# 1. given_movie_vector = a specific movie's embeddings. This will return top movies relating to that movie.\n",
    "# 2. given_movie_vector = user_profile's vector. This will return top movies recommended for this user.\n",
    "\n",
    "def get_top_movies_cosine(tfidf_matrix, given_movie_vector, movie_titles, top_n=5):\n",
    "\n",
    "    # Compute cosine similarity between the movie at movie_index and all movies in the matrix\n",
    "    cosine_similarities = get_cosine_similarity(given_movie_vector, tfidf_matrix).flatten()\n",
    "\n",
    "    # Get the indices of the top_n movies with the highest cosine similarity scores\n",
    "    # Use argsort and reverse it with [::-1] to get the indices in descending order of similarity\n",
    "    # Skip the first one as it is the movie itself with a similarity of 1\n",
    "    similar_indices = cosine_similarities.argsort()[::-1][1:top_n+1]\n",
    "\n",
    "    # Get the scores for the top_n movies\n",
    "    similar_scores = cosine_similarities[similar_indices]\n",
    "\n",
    "    # Combine indices and scores into a list of tuples and return\n",
    "    top_movies = [(movie_titles[index], index, score) for index, score in zip(similar_indices, similar_scores)]\n",
    "\n",
    "    print(f\"Top similar movies to the provided movie vector:\\n\")\n",
    "    for num, (title, index, score) in enumerate(top_movies, start = 1):\n",
    "        print(f\"{num}. \\\"{title}\\\" at ROW {index} with similarity score: {score}\")\n",
    "\n",
    "    return top_movies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i, title in enumerate(titles_with_synopsis):\n",
    "    print(i, title)\n",
    "\n",
    "get_top_movies_cosine(bert_embeddings_matrix, bert_embeddings_matrix[204].reshape(1, -1), titles_with_synopsis, 5)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
