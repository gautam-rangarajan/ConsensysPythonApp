{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory of the Jupyter notebook\n",
    "notebook_directory = os.getcwd()\n",
    "\n",
    "# Assuming the notebook is in the 'bin/' folder, add the parent directory to sys.path\n",
    "parent_directory = os.path.dirname(notebook_directory)\n",
    "sys.path.append(parent_directory)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "#Run the following commands on terminal:\n",
    "# conda install spacy\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I needed to download these files for word-edit functions like stopwords and lemmatization to work. \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#This is needed for removing names from the text (#todo)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hello World code for TF-IDF:\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Example documents\n",
    "documents = ['the sky is blue', 'the sun is bright', 'the sun in the sky is bright', 'we can see the shining sun, the bright sun']\n",
    "\n",
    "# Create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Tokenize and build vocab\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Compute cosine similarity between all pairs\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "#print(cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overall Recommender System:**\n",
    "\n",
    "Context: The current group preferences (filters), and overall movie data set + properties\n",
    "\n",
    "Input: All movies voted on by a user\n",
    "\n",
    "Outputs: Next M = 10 movies to recommend to the user. (Say M = 5 or 10, so the user doesn't have to wait for loading times after every vote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommender Algorithm:**\n",
    "\n",
    "Content-based filtering with TFIDF and Cosine Similarity\n",
    "\n",
    "1. Preprocess data:\n",
    "    - Get all movie overview strings\n",
    "    - Tokenize the strings (break into words)\n",
    "    - Clean up data not useful for comparison (stopwords, numbers, etc.)\n",
    "    - Stemming/ Lemmatization (reduce words to root form)\n",
    "    <p> <br> </p>\n",
    "2. TF-IDF vector of words:\n",
    "    - Convert all the descriptions into vectors using TF-IDF\n",
    "    - Convert categorical features like genre into binary features using one-hot encoding\n",
    "    - Normalize numerical features such as release year and user ratings to ensure they are on the same scale as other features (0-1)\n",
    "    - Combine all 3 into one total vector describing the movie\n",
    "    <p> <br> </p>\n",
    "3. Calculate user profile as a weighted average vector of the feature vectors of all liked movies so far. Should be same size as the vector for each movie.\n",
    "    - We could later introduce logic to use disliked movies in algorithm, though I don't think we should.\n",
    "    <p> <br> </p>\n",
    "4. Generate recommendations:\n",
    "    - Whenever user makes a vote: (or N votes, to be more efficient), recalculate user profile vector.\n",
    "    - Whenever client requests next M top movies: Calculate cosine similarity between current user profile and every candidate movie in database. Specifically, candidate movies = all movies matching group filters and not yet swiped by user.\n",
    "    - Time complexity = O(No. of movies x no. of features per movie). i.e. Linear time wrt total matrix size.\n",
    "    - Return the top M = 10 movies with highest cosine similarity.\n",
    "     <p> <br> </p>\n",
    "5. Handle new users who have not swiped yet:\n",
    "    - Initial recommendation just filters by group filters and sorts by IMDB ratings.\n",
    "    - Future versions can try to present a more diverse set of initial movies to get better user input, leading to better subsequent recommendations.\n",
    "    <p> <br> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load movie dataset\n",
    "df = pd.read_csv(\"../amf.csv\")\n",
    "\n",
    "df['original_title'] = df['original_title'].fillna('')\n",
    "df['overview'] = df['overview'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get string columns as lists. We won't use title for TF-IDF, just for verification purposes\n",
    "id = df['id'].tolist()\n",
    "titles = df['original_title'].tolist()\n",
    "overviews = df['overview'].tolist()\n",
    "\n",
    "print(overviews[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from imdb import Cinemagoer\n",
    "import timeit\n",
    "\n",
    "# Create an instance of the Cinemagoer class\n",
    "cg = Cinemagoer()\n",
    "\n",
    "# Function to get movie description by IMDb ID\n",
    "def get_movie_description(imdb_id):\n",
    "    # Get movie data by IMDb ID\n",
    "    start = timeit.default_timer()\n",
    "    movie = cg.get_movie(imdb_id)\n",
    "    end = timeit.default_timer()\n",
    "    print(\"get_movie_description took {} seconds to run\".format(end - start))\n",
    "    result = {}\n",
    "    for info in movie.current_info:\n",
    "        if info in movie:\n",
    "            result[info] = movie[info]\n",
    "    return result\n",
    "\n",
    "\n",
    "imdb_id = '0111161'  # Example IMDb ID 'tt0111161' for \"The Shawshank Redemption\"\n",
    "print(get_movie_description(imdb_id))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization stuff\n",
    "\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "    \n",
    "def lemmatize_sentence(sentence, lemmatizer):\n",
    "    # Tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = pos_tag(word_tokenize(sentence))  \n",
    "    # Tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            # if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            # else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to delete people's names from descriptions (like Harry, Ron, etc.)\n",
    "\n",
    "def remove_people_names(text):\n",
    "    # Create a spaCy document\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Generate a list of entities that are NOT people\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ != 'PERSON']\n",
    "    # Generate a list of entities that are people to replace them from the original text\n",
    "    people = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "\n",
    "    # Replace people's names with an empty string\n",
    "    for person in people:\n",
    "        text = text.replace(person, '')\n",
    "\n",
    "    # Rejoin entities that are not people to form the processed text\n",
    "    # This step may or may not be necessary based on how you want to use the result\n",
    "    #text = ' '.join(entities)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes stops, punctuations, digits, and double spaces.\n",
    "def remove_stops(text, stops):\n",
    "    words = text.split()\n",
    "    final = []\n",
    "    for word in words:\n",
    "        if word not in stops:\n",
    "            final.append(word)\n",
    "    final = \" \".join(final)\n",
    "    final = final.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    final = \"\".join([i for i in final if not i.isdigit()])\n",
    "    while \"  \" in final:\n",
    "        final = final.replace(\"  \", \" \")\n",
    "    return (final)\n",
    "\n",
    "\n",
    "#take in a list of strings and clean them up for use in TF-IDF\n",
    "def clean_docs(docs):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stops = stopwords.words(\"english\")\n",
    "    final = []\n",
    "    for doc in docs:\n",
    "        clean_doc = doc\n",
    "        #clean_doc = remove_people_names(doc)\n",
    "        clean_doc = lemmatize_sentence(clean_doc, lemmatizer)\n",
    "        clean_doc = remove_stops(clean_doc, stops)\n",
    "        #Handling weird issue where apostrophe-s ('s) --> s as separate words in cleaned version\n",
    "        clean_doc = clean_doc.replace(' s ', ' ')\n",
    "        final.append(clean_doc)\n",
    "    return (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FYI - Stop words that will be deleted by the remove_stops function:\n",
    "stops = stopwords.words(\"english\")\n",
    "print(stops)\n",
    "print(len(stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[10 mins to run] Get the cleaned overviews that will be fed into the TF-IDF function\n",
    "cleaned_overviews = clean_docs(overviews)\n",
    "print(cleaned_overviews[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_cleaned_synopsis_for_movies(imdb_movie_id):\n",
    "    imdb_movie_id = imdb_movie_id.replace(\"tt\", \"\")\n",
    "    movie_synopsis = get_movie_description(imdb_movie_id)[\"synopsis\"] #List of strings\n",
    "    return clean_docs(movie_synopsis)[0]\n",
    "\n",
    "imdb_movie_ids = [\"tt1457767\", \"tt0468569\"]\n",
    "cleaned_synopsis_list = [get_cleaned_synopsis_for_movies(imdb_movie_id) for imdb_movie_id in imdb_movie_ids]\n",
    "print(cleaned_synopsis_list)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate vectorizer model. Takes about 11 seconds\n",
    "vectorizer = TfidfVectorizer(\n",
    "                                lowercase=True,\n",
    "                                max_features= 5000,\n",
    "                                max_df=0.8,\n",
    "                                min_df=5,\n",
    "                                ngram_range = (1,3),\n",
    "                                stop_words = \"english\"\n",
    "\n",
    "                            )\n",
    "\n",
    "vectors = vectorizer.fit_transform(cleaned_overviews)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_vectors = vectors.toarray()\n",
    "df = pd.DataFrame(dense_vectors, columns=feature_names)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top values from TF-IDF tester\n",
    "\n",
    "top_values = df.iloc[892].sort_values(ascending=False)[:10]\n",
    "print(top_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectors[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "synopsis_vectors = vectorizer.fit_transform(cleaned_synopsis_list)\n",
    "synopsis_feature_names = vectorizer.get_feature_names_out()\n",
    "dense_synopsis_vectors = synopsis_vectors.toarray()\n",
    "synopsis_df = pd.DataFrame(dense_synopsis_vectors, columns=synopsis_feature_names)\n",
    "print(synopsis_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This calculates Cosines similarity between 2 vectors (movies).\n",
    "\n",
    "#Note: Cosine similarity expects 2D matrices. \n",
    "#To perform cosine similarity on vectors, remember to reshape the vector in the 2D shape (1, N), where N is the vector length.\n",
    "#to-do: Update this function to become a weighted cosine, using weights from a file.\n",
    "def get_cosine_similarity(movie_vector_1, movie_vector_2):\n",
    "\n",
    "    cosine_sim = cosine_similarity(movie_vector_1, movie_vector_2)\n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Cosine Similarity\n",
    "\n",
    "movie_vector_1 = vectors[0] #Toy Story\n",
    "movie_vector_2 = vectors[1] #Jumanji\n",
    "\n",
    "print(get_cosine_similarity(movie_vector_1, movie_vector_2))\n",
    "\n",
    "movie_vector_1 = vectors[4766] #Harry Potter 1 (TPS)\n",
    "movie_vector_2 = vectors[5678] #Harry Potter 2 (TCoS)\n",
    "\n",
    "print(get_cosine_similarity(movie_vector_1, movie_vector_2))\n",
    "\n",
    "movie_vector_1 = vectors[4766] #Harry Potter 1 (TPS)\n",
    "movie_vector_2 = vectors[892] #The Wizard of Oz\n",
    "print(get_cosine_similarity(movie_vector_1, movie_vector_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the top movies relating to a given movie vector using cosine similarity. \n",
    "#2 use cases for this:\n",
    "# 1. given_movie_vector = a specific movie's TF-IDF vector. This will return top movies relating to that movie.\n",
    "# 2. given_movie_vector = user_profile's vector. This will return top movies recommended for this user. \n",
    "\n",
    "def get_top_movies_cosine(tfidf_matrix, given_movie_vector, movie_titles, top_n=5):\n",
    "    \n",
    "    # Compute cosine similarity between the movie at movie_index and all movies in the matrix\n",
    "    cosine_similarities = get_cosine_similarity(given_movie_vector, tfidf_matrix).flatten()\n",
    "    \n",
    "    # Get the indices of the top_n movies with the highest cosine similarity scores\n",
    "    # Use argsort and reverse it with [::-1] to get the indices in descending order of similarity\n",
    "    # Skip the first one as it is the movie itself with a similarity of 1\n",
    "    similar_indices = cosine_similarities.argsort()[::-1][1:top_n+1]\n",
    "    \n",
    "    # Get the scores for the top_n movies\n",
    "    similar_scores = cosine_similarities[similar_indices]\n",
    "    \n",
    "    # Combine indices and scores into a list of tuples and return\n",
    "    top_movies = [(movie_titles[index], index, score) for index, score in zip(similar_indices, similar_scores)]\n",
    "\n",
    "    print(f\"Top similar movies to the provided movie vector:\\n\")\n",
    "    for num, (title, index, score) in enumerate(top_movies, start = 1):\n",
    "        print(f\"{num}. \\\"{title}\\\" at ROW {index} with similarity score: {score}\")\n",
    "\n",
    "    return top_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_movies_cosine(vectors, vectors[162], titles, 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate updated user profile after they have voted on M movies. \n",
    "# M = 1 means immediate feedback loop. But it may not be ideal. It might bias our recommendations towards our initial dataset (High exploit, low explore)\n",
    "# I think M = 5 or 10 might be better. \n",
    "# An even better idea is a hybrid of the above. M = 10 inititally, and after some votes M --> 1. \n",
    "\n",
    "def update_user_profile_batch(user_profile, movie_vectors, ratings, M):\n",
    "    \"\"\"\n",
    "    Update the user profile based on a batch of movie ratings.\n",
    "\n",
    "    :param user_profile: scipy.sparse matrix, the current user profile vector (1, N)\n",
    "    :param movie_vectors: list of scipy.sparse matrices, the TF-IDF vectors of the rated movies [(1, N), (1, N), ...]\n",
    "    :param ratings: list of str, the ratings for each movie ('like' or 'dislike')\n",
    "    :param M: int, the number of ratings to process before updating the profile\n",
    "    :return: scipy.sparse matrix, the updated user profile vector (1, N)\n",
    "    \"\"\"\n",
    "    dislike_factor = 1/3 #we can tweak this to see impact on recommendations. \n",
    "\n",
    "    if len(movie_vectors) != len(ratings):\n",
    "        raise ValueError(\"The number of movie vectors and ratings must be the same\")\n",
    "\n",
    "    if len(movie_vectors) < M:\n",
    "        raise ValueError(\"The number of movie vectors must be at least M\")\n",
    "\n",
    "    # Initialize a temporary profile change vector\n",
    "    profile_change = csr_matrix((1, user_profile.shape[1]))\n",
    "\n",
    "    # Process each movie vector and rating\n",
    "    for movie_vector, rating in zip(movie_vectors, ratings):\n",
    "        if rating == 'like':\n",
    "            profile_change += movie_vector\n",
    "        elif rating == 'dislike':\n",
    "            profile_change -= (dislike_factor * movie_vector)\n",
    "        else:\n",
    "            raise ValueError(\"Rating must be 'like' or 'dislike'\")\n",
    "\n",
    "    # Update the user profile after processing M ratings\n",
    "    updated_profile = user_profile + profile_change\n",
    "\n",
    "    # Normalize the updated profile\n",
    "    updated_profile = normalize(updated_profile, norm='l2', axis=1)\n",
    "\n",
    "    return updated_profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example usage of User Profile Update:\n",
    "\n",
    "# In our app, we should initialize user_profile as a 1-D sparse matrix of zeros when the User() is created.\n",
    "# i.e. user_profile should be a property of the User() object.\n",
    "\n",
    "VECTOR_LENGTH = vectors.shape[1] #This could be assigned as a global variable. Once we settle on an algorithm, this should not change. \n",
    "\n",
    "user_profile = csr_matrix((1, VECTOR_LENGTH)) #Sparse matrix for quick maths. (e.g. 2 + 2 is 4. Minus 1 that's 3)\n",
    "print(type(user_profile), user_profile.shape)\n",
    "\n",
    "movie_vectors = [vectors[i] for i in range(5)]  # Replace with actual indices of movies the user rated\n",
    "ratings = ['like', 'dislike', 'like', 'like', 'dislike']  # Example ratings\n",
    "\n",
    "#For display purposes:\n",
    "print('Displaying rated movies:')\n",
    "for i, _ in enumerate(movie_vectors):\n",
    "    print(f\"{i}. {titles[i]} - {ratings[i]}\")\n",
    "\n",
    "# Update the profile based on user ratings of M movies\n",
    "M = 5\n",
    "user_profile = update_user_profile_batch(user_profile, movie_vectors, ratings, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that the user profile has been updated, get the top 10 recommendations for this user:\n",
    "print(get_top_movies_cosine(vectors, user_profile, titles, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"testing git merge conflicts after removing cell outputs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
