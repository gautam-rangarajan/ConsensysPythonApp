{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory of the Jupyter notebook\n",
    "notebook_directory = os.getcwd()\n",
    "\n",
    "# Assuming the notebook is in the 'bin/' folder, add the parent directory to sys.path\n",
    "parent_directory = os.path.dirname(notebook_directory)\n",
    "sys.path.append(parent_directory)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "#Run the following commands on terminal:\n",
    "# conda install spacy\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\detab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\detab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\detab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\detab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#I needed to download these files for word-edit functions like stopwords and lemmatization to work. \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#This is needed for removing names from the text (#todo)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hello World code for TF-IDF:\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Example documents\n",
    "documents = ['the sky is blue', 'the sun is bright', 'the sun in the sky is bright', 'we can see the shining sun, the bright sun']\n",
    "\n",
    "# Create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Tokenize and build vocab\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Compute cosine similarity between all pairs\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "#print(cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overall Recommender System:**\n",
    "\n",
    "Context: The current group preferences (filters), and overall movie data set + properties\n",
    "\n",
    "Input: All movies voted on by a user\n",
    "\n",
    "Outputs: Next M = 10 movies to recommend to the user. (Say M = 5 or 10, so the user doesn't have to wait for loading times after every vote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommender Algorithm:**\n",
    "\n",
    "Content-based filtering with TFIDF and Cosine Similarity\n",
    "\n",
    "1. Preprocess data:\n",
    "    - Get all movie overview strings\n",
    "    - Tokenize the strings (break into words)\n",
    "    - Clean up data not useful for comparison (stopwords, numbers, etc.)\n",
    "    - Stemming/ Lemmatization (reduce words to root form)\n",
    "    <p> <br> </p>\n",
    "2. TF-IDF vector of words:\n",
    "    - Convert all the descriptions into vectors using TF-IDF\n",
    "    - Convert categorical features like genre into binary features using one-hot encoding\n",
    "    - Normalize numerical features such as release year and user ratings to ensure they are on the same scale as other features (0-1)\n",
    "    - Combine all 3 into one total vector describing the movie\n",
    "    <p> <br> </p>\n",
    "3. Calculate user profile as a weighted average vector of the feature vectors of all liked movies so far. Should be same size as the vector for each movie.\n",
    "    - We could later introduce logic to use disliked movies in algorithm, though I don't think we should.\n",
    "    <p> <br> </p>\n",
    "4. Generate recommendations:\n",
    "    - Whenever user makes a vote: (or N votes, to be more efficient), recalculate user profile vector.\n",
    "    - Whenever client requests next M top movies: Calculate cosine similarity between current user profile and every candidate movie in database. Specifically, candidate movies = all movies matching group filters and not yet swiped by user.\n",
    "    - Time complexity = O(No. of movies x no. of features per movie). i.e. Linear time wrt total matrix size.\n",
    "    - Return the top M = 10 movies with highest cosine similarity.\n",
    "     <p> <br> </p>\n",
    "5. Handle new users who have not swiped yet:\n",
    "    - Initial recommendation just filters by group filters and sorts by IMDB ratings.\n",
    "    - Future versions can try to present a more diverse set of initial movies to get better user input, leading to better subsequent recommendations.\n",
    "    <p> <br> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load movie dataset\n",
    "df = pd.read_csv(\"../amf.csv\")\n",
    "\n",
    "df['original_title'] = df['original_title'].fillna('')\n",
    "df['overview'] = df['overview'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Led by Woody, Andy's toys live happily in his room until Andy's birthday brings Buzz Lightyear onto the scene. Afraid of losing his place in Andy's heart, Woody plots against Buzz. But when circumstances separate Buzz and Woody from their owner, the duo eventually learns to put aside their differences.\", \"When siblings Judy and Peter discover an enchanted board game that opens the door to a magical world, they unwittingly invite Alan -- an adult who's been trapped inside the game for 26 years -- into their living room. Alan's only hope for freedom is to finish the game, which proves risky as all three find themselves running from giant rhinoceroses, evil monkeys and other terrifying creatures.\", \"A family wedding reignites the ancient feud between next-door neighbors and fishing buddies John and Max. Meanwhile, a sultry Italian divorcée opens a restaurant at the local bait shop, alarming the locals who worry she'll scare the fish away. But she's less interested in seafood than she is in cooking up a hot time with Max.\", 'Cheated on, mistreated and stepped on, the women are holding their breath, waiting for the elusive \"good man\" to break a string of less-than-stellar lovers. Friends and confidants Vannah, Bernie, Glo and Robin talk it all out, determined to find a better way to breathe.', \"Just when George Banks has recovered from his daughter's wedding, he receives the news that she's pregnant ... and that George's wife, Nina, is expecting too. He was planning on selling their home, but that's a plan that -- like George -- will have to change with the arrival of both a grandchild and a kid of his own.\"]\n"
     ]
    }
   ],
   "source": [
    "#Get string columns as lists. We won't use title for TF-IDF, just for verification purposes\n",
    "id = df['id'].tolist()\n",
    "titles = df['original_title'].tolist()\n",
    "overviews = df['overview'].tolist()\n",
    "\n",
    "print(overviews[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization stuff\n",
    "\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "    \n",
    "def lemmatize_sentence(sentence, lemmatizer):\n",
    "    # Tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = pos_tag(word_tokenize(sentence))  \n",
    "    # Tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            # if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            # else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to delete people's names from descriptions (like Harry, Ron, etc.)\n",
    "\n",
    "def remove_people_names(text):\n",
    "    # Create a spaCy document\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Generate a list of entities that are NOT people\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ != 'PERSON']\n",
    "    # Generate a list of entities that are people to replace them from the original text\n",
    "    people = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "\n",
    "    # Replace people's names with an empty string\n",
    "    for person in people:\n",
    "        text = text.replace(person, '')\n",
    "\n",
    "    # Rejoin entities that are not people to form the processed text\n",
    "    # This step may or may not be necessary based on how you want to use the result\n",
    "    #text = ' '.join(entities)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes stops, punctuations, digits, and double spaces.\n",
    "def remove_stops(text, stops):\n",
    "    words = text.split()\n",
    "    final = []\n",
    "    for word in words:\n",
    "        if word not in stops:\n",
    "            final.append(word)\n",
    "    final = \" \".join(final)\n",
    "    final = final.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    final = \"\".join([i for i in final if not i.isdigit()])\n",
    "    while \"  \" in final:\n",
    "        final = final.replace(\"  \", \" \")\n",
    "    return (final)\n",
    "\n",
    "\n",
    "#take in a list of strings and clean them up for use in TF-IDF\n",
    "def clean_docs(docs):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stops = stopwords.words(\"english\")\n",
    "    final = []\n",
    "    for doc in docs:\n",
    "        clean_doc = doc\n",
    "        #clean_doc = remove_people_names(doc)\n",
    "        clean_doc = lemmatize_sentence(clean_doc, lemmatizer)\n",
    "        clean_doc = remove_stops(clean_doc, stops)\n",
    "        #Handling weird issue where apostrophe-s ('s) --> s as separate words in cleaned version\n",
    "        clean_doc = clean_doc.replace(' s ', ' ')\n",
    "        final.append(clean_doc)\n",
    "    return (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "#FYI - Stop words that will be deleted by the remove_stops function:\n",
    "stops = stopwords.words(\"english\")\n",
    "print(stops)\n",
    "print(len(stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Led Woody Andy toy live happily room Andy birthday bring Buzz Lightyear onto scene Afraid lose place Andy heart Woody plots Buzz But circumstance separate Buzz Woody owner duo eventually learn put aside difference ', 'When sibling Judy Peter discover enchant board game open door magical world unwittingly invite Alan adult trap inside game year living room Alan hope freedom finish game prove risky three find run giant rhinoceros evil monkey terrifying creature ', 'A family wedding reignite ancient feud nextdoor neighbor fishing buddy John Max Meanwhile sultry Italian divorcée open restaurant local bait shop alarm local worry ll scare fish away But less interested seafood cook hot time Max ', 'Cheated mistreat step woman hold breath wait elusive good man break string lessthanstellar lover Friends confidant Vannah Bernie Glo Robin talk determine find good way breathe ', 'Just George Banks recover daughter wedding receive news pregnant George wife Nina expect He plan sell home plan like George change arrival grandchild kid ']\n"
     ]
    }
   ],
   "source": [
    "#[10 mins to run] Get the cleaned overviews that will be fed into the TF-IDF function\n",
    "cleaned_overviews = clean_docs(overviews)\n",
    "print(cleaned_overviews[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate vectorizer model. Takes about 11 seconds\n",
    "vectorizer = TfidfVectorizer(\n",
    "                                lowercase=True,\n",
    "                                max_features= 5000,\n",
    "                                max_df=0.8,\n",
    "                                min_df=5,\n",
    "                                ngram_range = (1,3),\n",
    "                                stop_words = \"english\"\n",
    "\n",
    "                            )\n",
    "\n",
    "vectors = vectorizer.fit_transform(cleaned_overviews)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       aaron  abandon  abandoned  abby  abduct  ability  able  aboard  \\\n",
      "0        0.0      0.0        0.0   0.0     0.0      0.0   0.0     0.0   \n",
      "1        0.0      0.0        0.0   0.0     0.0      0.0   0.0     0.0   \n",
      "2        0.0      0.0        0.0   0.0     0.0      0.0   0.0     0.0   \n",
      "3        0.0      0.0        0.0   0.0     0.0      0.0   0.0     0.0   \n",
      "4        0.0      0.0        0.0   0.0     0.0      0.0   0.0     0.0   \n",
      "...      ...      ...        ...   ...     ...      ...   ...     ...   \n",
      "45461    0.0      0.0        0.0   0.0     0.0      0.0   0.0     0.0   \n",
      "45462    0.0      0.0        0.0   0.0     0.0      0.0   0.0     0.0   \n",
      "45463    0.0      0.0        0.0   0.0     0.0      0.0   0.0     0.0   \n",
      "45464    0.0      0.0        0.0   0.0     0.0      0.0   0.0     0.0   \n",
      "45465    0.0      0.0        0.0   0.0     0.0      0.0   0.0     0.0   \n",
      "\n",
      "       abortion  abroad  ...  youth  youthful   yu  zatoichi  zealand  zero  \\\n",
      "0           0.0     0.0  ...    0.0       0.0  0.0       0.0      0.0   0.0   \n",
      "1           0.0     0.0  ...    0.0       0.0  0.0       0.0      0.0   0.0   \n",
      "2           0.0     0.0  ...    0.0       0.0  0.0       0.0      0.0   0.0   \n",
      "3           0.0     0.0  ...    0.0       0.0  0.0       0.0      0.0   0.0   \n",
      "4           0.0     0.0  ...    0.0       0.0  0.0       0.0      0.0   0.0   \n",
      "...         ...     ...  ...    ...       ...  ...       ...      ...   ...   \n",
      "45461       0.0     0.0  ...    0.0       0.0  0.0       0.0      0.0   0.0   \n",
      "45462       0.0     0.0  ...    0.0       0.0  0.0       0.0      0.0   0.0   \n",
      "45463       0.0     0.0  ...    0.0       0.0  0.0       0.0      0.0   0.0   \n",
      "45464       0.0     0.0  ...    0.0       0.0  0.0       0.0      0.0   0.0   \n",
      "45465       0.0     0.0  ...    0.0       0.0  0.0       0.0      0.0   0.0   \n",
      "\n",
      "       zoe  zombie  zone  zoo  \n",
      "0      0.0     0.0   0.0  0.0  \n",
      "1      0.0     0.0   0.0  0.0  \n",
      "2      0.0     0.0   0.0  0.0  \n",
      "3      0.0     0.0   0.0  0.0  \n",
      "4      0.0     0.0   0.0  0.0  \n",
      "...    ...     ...   ...  ...  \n",
      "45461  0.0     0.0   0.0  0.0  \n",
      "45462  0.0     0.0   0.0  0.0  \n",
      "45463  0.0     0.0   0.0  0.0  \n",
      "45464  0.0     0.0   0.0  0.0  \n",
      "45465  0.0     0.0   0.0  0.0  \n",
      "\n",
      "[45466 rows x 5000 columns]\n"
     ]
    }
   ],
   "source": [
    "dense_vectors = vectors.toarray()\n",
    "df = pd.DataFrame(dense_vectors, columns=feature_names)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thing          0.316296\n",
      "yellow         0.271024\n",
      "make friend    0.265093\n",
      "dorothy        0.263323\n",
      "wicked         0.261096\n",
      "lion           0.256555\n",
      "wizard         0.251303\n",
      "make way       0.240276\n",
      "make           0.236940\n",
      "witch          0.223544\n",
      "Name: 892, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Top values from TF-IDF tester\n",
    "\n",
    "top_values = df.iloc[892].sort_values(ascending=False)[:10]\n",
    "print(top_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1201)\t0.19953064420228217\n",
      "  (0, 253)\t0.228135251450199\n",
      "  (0, 2521)\t0.13672908683661142\n",
      "  (0, 1509)\t0.16180787148930936\n",
      "  (0, 1334)\t0.2058782082915691\n",
      "  (0, 3179)\t0.15628659156625843\n",
      "  (0, 3951)\t0.1814768168092712\n",
      "  (0, 738)\t0.19322501352486027\n",
      "  (0, 2028)\t0.15470588341737868\n",
      "  (0, 3312)\t0.1348644698142803\n",
      "  (0, 2638)\t0.13355872379188607\n",
      "  (0, 81)\t0.2184228005546969\n",
      "  (0, 3873)\t0.162179038005134\n",
      "  (0, 523)\t0.13087873107203457\n",
      "  (0, 433)\t0.19162174426761105\n",
      "  (0, 3790)\t0.16951345675152515\n",
      "  (0, 1984)\t0.20704015258040712\n",
      "  (0, 2599)\t0.11169821423324114\n",
      "  (0, 4535)\t0.21393290725764616\n",
      "  (0, 165)\t0.6391564300822646\n",
      "  (1, 996)\t0.15563187815440185\n",
      "  (1, 4441)\t0.19134065423614086\n",
      "  (1, 2904)\t0.20918469376033463\n",
      "  (1, 1514)\t0.13334559243126776\n",
      "  (1, 1870)\t0.16715962965957554\n",
      "  :\t:\n",
      "  (8, 3968)\t0.19189179478322363\n",
      "  (8, 4484)\t0.14775784938517658\n",
      "  (8, 4357)\t0.19593045192860659\n",
      "  (8, 4406)\t0.1148077691392191\n",
      "  (8, 4716)\t0.31727749237618247\n",
      "  (8, 756)\t0.19699223086722104\n",
      "  (8, 2360)\t0.16394480549731624\n",
      "  (8, 4332)\t0.18513134173596626\n",
      "  (8, 40)\t0.13162218050720162\n",
      "  (8, 2289)\t0.1424789245580221\n",
      "  (8, 1433)\t0.10486718985729548\n",
      "  (8, 3315)\t0.10982704835877465\n",
      "  (8, 1074)\t0.20445839643657893\n",
      "  (8, 1839)\t0.2535195983300642\n",
      "  (9, 527)\t0.35166870029340924\n",
      "  (9, 3720)\t0.25630338985491746\n",
      "  (9, 1177)\t0.3471301941165099\n",
      "  (9, 4835)\t0.30999543402415114\n",
      "  (9, 2516)\t0.26375283449829784\n",
      "  (9, 3423)\t0.30723472094643145\n",
      "  (9, 4379)\t0.3888844683827087\n",
      "  (9, 2020)\t0.23834712329489444\n",
      "  (9, 2956)\t0.22862956752859145\n",
      "  (9, 477)\t0.29929551678899796\n",
      "  (9, 2345)\t0.28318308368996686\n"
     ]
    }
   ],
   "source": [
    "print(vectors[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This calculates Cosines similarity between 2 vectors (movies).\n",
    "\n",
    "#Note: Cosine similarity expects 2D matrices. \n",
    "#To perform cosine similarity on vectors, remember to reshape the vector in the 2D shape (1, N), where N is the vector length.\n",
    "#to-do: Update this function to become a weighted cosine, using weights from a file.\n",
    "def get_cosine_similarity(movie_vector_1, movie_vector_2):\n",
    "\n",
    "    cosine_sim = cosine_similarity(movie_vector_1, movie_vector_2)\n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02594483]]\n",
      "[[0.15380733]]\n",
      "[[0.0632266]]\n"
     ]
    }
   ],
   "source": [
    "#Testing Cosine Similarity\n",
    "\n",
    "movie_vector_1 = vectors[0] #Toy Story\n",
    "movie_vector_2 = vectors[1] #Jumanji\n",
    "\n",
    "print(get_cosine_similarity(movie_vector_1, movie_vector_2))\n",
    "\n",
    "movie_vector_1 = vectors[4766] #Harry Potter 1 (TPS)\n",
    "movie_vector_2 = vectors[5678] #Harry Potter 2 (TCoS)\n",
    "\n",
    "print(get_cosine_similarity(movie_vector_1, movie_vector_2))\n",
    "\n",
    "movie_vector_1 = vectors[4766] #Harry Potter 1 (TPS)\n",
    "movie_vector_2 = vectors[892] #The Wizard of Oz\n",
    "print(get_cosine_similarity(movie_vector_1, movie_vector_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the top movies relating to a given movie vector using cosine similarity. \n",
    "#2 use cases for this:\n",
    "# 1. given_movie_vector = a specific movie's TF-IDF vector. This will return top movies relating to that movie.\n",
    "# 2. given_movie_vector = user_profile's vector. This will return top movies recommended for this user. \n",
    "\n",
    "def get_top_movies_cosine(tfidf_matrix, given_movie_vector, movie_titles, top_n=5):\n",
    "    \n",
    "    # Compute cosine similarity between the movie at movie_index and all movies in the matrix\n",
    "    cosine_similarities = get_cosine_similarity(given_movie_vector, tfidf_matrix).flatten()\n",
    "    \n",
    "    # Get the indices of the top_n movies with the highest cosine similarity scores\n",
    "    # Use argsort and reverse it with [::-1] to get the indices in descending order of similarity\n",
    "    # Skip the first one as it is the movie itself with a similarity of 1\n",
    "    similar_indices = cosine_similarities.argsort()[::-1][1:top_n+1]\n",
    "    \n",
    "    # Get the scores for the top_n movies\n",
    "    similar_scores = cosine_similarities[similar_indices]\n",
    "    \n",
    "    # Combine indices and scores into a list of tuples and return\n",
    "    top_movies = [(movie_titles[index], index, score) for index, score in zip(similar_indices, similar_scores)]\n",
    "\n",
    "    print(f\"Top similar movies to the provided movie vector:\\n\")\n",
    "    for num, (title, index, score) in enumerate(top_movies, start = 1):\n",
    "        print(f\"{num}. \\\"{title}\\\" at ROW {index} with similarity score: {score}\")\n",
    "\n",
    "    return top_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top similar movies to the provided movie vector:\n",
      "\n",
      "1. \"Broadway Melody of 1940\" at ROW 10175 with similarity score: 0.3059949855886764\n",
      "2. \"If These Knishes Could Talk: The Story of the NY Accent\" at ROW 37657 with similarity score: 0.2521433823774023\n",
      "3. \"The Transfiguration\" at ROW 43275 with similarity score: 0.2515363367569358\n",
      "4. \"Les Ripoux\" at ROW 44766 with similarity score: 0.2486966593060226\n",
      "5. \"Khiladi 786\" at ROW 40343 with similarity score: 0.24721219021926344\n",
      "6. \"Loose Cannons\" at ROW 6411 with similarity score: 0.2472099680754719\n",
      "7. \"Texas Killing Fields\" at ROW 19018 with similarity score: 0.24454771954582263\n",
      "8. \"Shoot the Moon\" at ROW 5989 with similarity score: 0.2262166595812431\n",
      "9. \"Strictly Ballroom\" at ROW 1147 with similarity score: 0.22098725761853977\n",
      "10. \"Alvin and the Chipmunks: The Road Chip\" at ROW 38589 with similarity score: 0.2112776847969979\n"
     ]
    }
   ],
   "source": [
    "get_top_movies_cosine(vectors, vectors[162], titles, 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate updated user profile after they have voted on M movies. \n",
    "# M = 1 means immediate feedback loop. But it may not be ideal. It might bias our recommendations towards our initial dataset (High exploit, low explore)\n",
    "# I think M = 5 or 10 might be better. \n",
    "# An even better idea is a hybrid of the above. M = 10 inititally, and after some votes M --> 1. \n",
    "\n",
    "def update_user_profile_batch(user_profile, movie_vectors, ratings, M):\n",
    "    \"\"\"\n",
    "    Update the user profile based on a batch of movie ratings.\n",
    "\n",
    "    :param user_profile: scipy.sparse matrix, the current user profile vector (1, N)\n",
    "    :param movie_vectors: list of scipy.sparse matrices, the TF-IDF vectors of the rated movies [(1, N), (1, N), ...]\n",
    "    :param ratings: list of str, the ratings for each movie ('like' or 'dislike')\n",
    "    :param M: int, the number of ratings to process before updating the profile\n",
    "    :return: scipy.sparse matrix, the updated user profile vector (1, N)\n",
    "    \"\"\"\n",
    "    dislike_factor = 1/3 #we can tweak this to see impact on recommendations. \n",
    "\n",
    "    if len(movie_vectors) != len(ratings):\n",
    "        raise ValueError(\"The number of movie vectors and ratings must be the same\")\n",
    "\n",
    "    if len(movie_vectors) < M:\n",
    "        raise ValueError(\"The number of movie vectors must be at least M\")\n",
    "\n",
    "    # Initialize a temporary profile change vector\n",
    "    profile_change = csr_matrix((1, user_profile.shape[1]))\n",
    "\n",
    "    # Process each movie vector and rating\n",
    "    for movie_vector, rating in zip(movie_vectors, ratings):\n",
    "        if rating == 'like':\n",
    "            profile_change += movie_vector\n",
    "        elif rating == 'dislike':\n",
    "            profile_change -= (dislike_factor * movie_vector)\n",
    "        else:\n",
    "            raise ValueError(\"Rating must be 'like' or 'dislike'\")\n",
    "\n",
    "    # Update the user profile after processing M ratings\n",
    "    updated_profile = user_profile + profile_change\n",
    "\n",
    "    # Normalize the updated profile\n",
    "    updated_profile = normalize(updated_profile, norm='l2', axis=1)\n",
    "\n",
    "    return updated_profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'> (1, 5000)\n",
      "Displaying rated movies:\n",
      "0. Toy Story - like\n",
      "1. Jumanji - dislike\n",
      "2. Grumpier Old Men - like\n",
      "3. Waiting to Exhale - like\n",
      "4. Father of the Bride Part II - dislike\n"
     ]
    }
   ],
   "source": [
    "#Example usage of User Profile Update:\n",
    "\n",
    "# In our app, we should initialize user_profile as a 1-D sparse matrix of zeros when the User() is created.\n",
    "# i.e. user_profile should be a property of the User() object.\n",
    "\n",
    "VECTOR_LENGTH = vectors.shape[1] #This could be assigned as a global variable. Once we settle on an algorithm, this should not change. \n",
    "\n",
    "user_profile = csr_matrix((1, VECTOR_LENGTH)) #Sparse matrix for quick maths. (e.g. 2 + 2 is 4. Minus 1 that's 3)\n",
    "print(type(user_profile), user_profile.shape)\n",
    "\n",
    "movie_vectors = [vectors[i] for i in range(5)]  # Replace with actual indices of movies the user rated\n",
    "ratings = ['like', 'dislike', 'like', 'like', 'dislike']  # Example ratings\n",
    "\n",
    "#For display purposes:\n",
    "print('Displaying rated movies:')\n",
    "for i, _ in enumerate(movie_vectors):\n",
    "    print(f\"{i}. {titles[i]} - {ratings[i]}\")\n",
    "\n",
    "# Update the profile based on user ratings of M movies\n",
    "M = 5\n",
    "user_profile = update_user_profile_batch(user_profile, movie_vectors, ratings, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top similar movies to the provided movie vector:\n",
      "\n",
      "1. \"Toy Story\" at ROW 0 with similarity score: 0.5573292945991553\n",
      "2. \"Grumpier Old Men\" at ROW 2 with similarity score: 0.550677658042087\n",
      "3. \"The 40 Year Old Virgin\" at ROW 10301 with similarity score: 0.2886061333063643\n",
      "4. \"The Champ\" at ROW 8327 with similarity score: 0.28021104108161937\n",
      "5. \"Toy Story 3\" at ROW 15348 with similarity score: 0.2789042331047257\n",
      "6. \"Andy Kaufman Plays Carnegie Hall\" at ROW 43427 with similarity score: 0.26488180074963247\n",
      "7. \"Andy Hardy's Blonde Trouble\" at ROW 23843 with similarity score: 0.25113475128272983\n",
      "8. \"Superstar: The Life and Times of Andy Warhol\" at ROW 38476 with similarity score: 0.24818528564740497\n",
      "9. \"Andy Peters: Exclamation Mark Question Point\" at ROW 42721 with similarity score: 0.23805647569731256\n",
      "10. \"桃姐\" at ROW 20326 with similarity score: 0.22845267824390414\n",
      "[('Toy Story', 0, 0.5573292945991553), ('Grumpier Old Men', 2, 0.550677658042087), ('The 40 Year Old Virgin', 10301, 0.2886061333063643), ('The Champ', 8327, 0.28021104108161937), ('Toy Story 3', 15348, 0.2789042331047257), ('Andy Kaufman Plays Carnegie Hall', 43427, 0.26488180074963247), (\"Andy Hardy's Blonde Trouble\", 23843, 0.25113475128272983), ('Superstar: The Life and Times of Andy Warhol', 38476, 0.24818528564740497), ('Andy Peters: Exclamation Mark Question Point', 42721, 0.23805647569731256), ('桃姐', 20326, 0.22845267824390414)]\n"
     ]
    }
   ],
   "source": [
    "#Now that the user profile has been updated, get the top 10 recommendations for this user:\n",
    "print(get_top_movies_cosine(vectors, user_profile, titles, 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
