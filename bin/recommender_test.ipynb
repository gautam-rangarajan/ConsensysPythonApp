{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory of the Jupyter notebook\n",
    "notebook_directory = os.getcwd()\n",
    "\n",
    "# Assuming the notebook is in the 'bin/' folder, add the parent directory to sys.path\n",
    "parent_directory = os.path.dirname(notebook_directory)\n",
    "sys.path.append(parent_directory)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "#Run the following commands on conda:\n",
    "# conda install spacy\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\detab\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\detab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\detab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\detab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\detab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#I needed to download these files for word-edit functions like stopwords and lemmatization to work. \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#This is needed for removing names from the text (#todo)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hello World code for TF-IDF:\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Example documents\n",
    "documents = ['the sky is blue', 'the sun is bright', 'the sun in the sky is bright', 'we can see the shining sun, the bright sun']\n",
    "\n",
    "# Create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Tokenize and build vocab\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Compute cosine similarity between all pairs\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "#print(cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overall Recommender System:**\n",
    "\n",
    "Context: The current group preferences (filters), and overall movie data set + properties\n",
    "\n",
    "Input: All movies voted on by a user\n",
    "\n",
    "Outputs: Next M = 10 movies to recommend to the user. (Say M = 5 or 10, so the user doesn't have to wait for loading times after every vote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommender Algorithm:**\n",
    "\n",
    "Content-based filtering with TFIDF and Cosine Similarity\n",
    "\n",
    "1. Preprocess data:\n",
    "    - Get all movie overview strings\n",
    "    - Tokenize the strings (break into words)\n",
    "    - Clean up data not useful for comparison (stopwords, numbers, etc.)\n",
    "    - Stemming/ Lemmatization (reduce words to root form)\n",
    "    <p> <br> </p>\n",
    "2. TF-IDF vector of words:\n",
    "    - Convert all the descriptions into vectors using TF-IDF\n",
    "    - Convert categorical features like genre into binary features using one-hot encoding\n",
    "    - Normalize numerical features such as release year and user ratings to ensure they are on the same scale as other features (0-1)\n",
    "    - Combine all 3 into one total vector describing the movie\n",
    "    <p> <br> </p>\n",
    "3. Calculate user profile as a weighted average vector of the feature vectors of all liked movies so far. Should be same size as the vector for each movie.\n",
    "    - We could later introduce logic to use disliked movies in algorithm, though I don't think we should.\n",
    "    <p> <br> </p>\n",
    "4. Generate recommendations:\n",
    "    - Whenever user makes a vote: (or N votes, to be more efficient), recalculate user profile vector.\n",
    "    - Whenever client requests next M top movies: Calculate cosine similarity between current user profile and every candidate movie in database. Specifically, candidate movies = all movies matching group filters and not yet swiped by user.\n",
    "    - Time complexity = O(No. of movies x no. of features per movie). i.e. Linear time wrt total matrix size.\n",
    "    - Return the top M = 10 movies with highest cosine similarity.\n",
    "     <p> <br> </p>\n",
    "5. Handle new users who have not swiped yet:\n",
    "    - Initial recommendation just filters by group filters and sorts by IMDB ratings.\n",
    "    - Future versions can try to present a more diverse set of initial movies to get better user input, leading to better subsequent recommendations.\n",
    "    <p> <br> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load movie dataset\n",
    "df = pd.read_csv(\"../amf.csv\")\n",
    "\n",
    "df['original_title'] = df['original_title'].fillna('')\n",
    "df['overview'] = df['overview'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Led by Woody, Andy's toys live happily in his room until Andy's birthday brings Buzz Lightyear onto the scene. Afraid of losing his place in Andy's heart, Woody plots against Buzz. But when circumstances separate Buzz and Woody from their owner, the duo eventually learns to put aside their differences.\", \"When siblings Judy and Peter discover an enchanted board game that opens the door to a magical world, they unwittingly invite Alan -- an adult who's been trapped inside the game for 26 years -- into their living room. Alan's only hope for freedom is to finish the game, which proves risky as all three find themselves running from giant rhinoceroses, evil monkeys and other terrifying creatures.\", \"A family wedding reignites the ancient feud between next-door neighbors and fishing buddies John and Max. Meanwhile, a sultry Italian divorcée opens a restaurant at the local bait shop, alarming the locals who worry she'll scare the fish away. But she's less interested in seafood than she is in cooking up a hot time with Max.\", 'Cheated on, mistreated and stepped on, the women are holding their breath, waiting for the elusive \"good man\" to break a string of less-than-stellar lovers. Friends and confidants Vannah, Bernie, Glo and Robin talk it all out, determined to find a better way to breathe.', \"Just when George Banks has recovered from his daughter's wedding, he receives the news that she's pregnant ... and that George's wife, Nina, is expecting too. He was planning on selling their home, but that's a plan that -- like George -- will have to change with the arrival of both a grandchild and a kid of his own.\"]\n"
     ]
    }
   ],
   "source": [
    "#Get string columns as lists. We won't use title for TF-IDF, just for verification purposes\n",
    "id = df['id'].tolist()\n",
    "titles = df['original_title'].tolist()\n",
    "overviews = df['overview'].tolist()\n",
    "\n",
    "print(overviews[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization stuff\n",
    "\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "    \n",
    "def lemmatize_sentence(sentence, lemmatizer):\n",
    "    # Tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = pos_tag(word_tokenize(sentence))  \n",
    "    # Tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            # if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            # else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to delete people's names from descriptions (like Harry, Ron, etc.)\n",
    "\n",
    "def remove_people_names(text):\n",
    "    # Create a spaCy document\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Generate a list of entities that are NOT people\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ != 'PERSON']\n",
    "    # Generate a list of entities that are people to replace them from the original text\n",
    "    people = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "\n",
    "    # Replace people's names with an empty string\n",
    "    for person in people:\n",
    "        text = text.replace(person, '')\n",
    "\n",
    "    # Rejoin entities that are not people to form the processed text\n",
    "    # This step may or may not be necessary based on how you want to use the result\n",
    "    #text = ' '.join(entities)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes stops, punctuations, digits, and double spaces.\n",
    "def remove_stops(text, stops):\n",
    "    words = text.split()\n",
    "    final = []\n",
    "    for word in words:\n",
    "        if word not in stops:\n",
    "            final.append(word)\n",
    "    final = \" \".join(final)\n",
    "    final = final.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    final = \"\".join([i for i in final if not i.isdigit()])\n",
    "    while \"  \" in final:\n",
    "        final = final.replace(\"  \", \" \")\n",
    "    return (final)\n",
    "\n",
    "\n",
    "#take in a list of strings and clean them up for use in TF-IDF\n",
    "def clean_docs(docs):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stops = stopwords.words(\"english\")\n",
    "    final = []\n",
    "    for doc in docs:\n",
    "        clean_doc = remove_people_names(doc)\n",
    "        clean_doc = lemmatize_sentence(clean_doc, lemmatizer)\n",
    "        clean_doc = remove_stops(clean_doc, stops)\n",
    "        #Handling weird issue where apostrophe-s ('s) --> s as separate words in cleaned version\n",
    "        clean_doc = clean_doc.replace(' s ', ' ')\n",
    "        final.append(clean_doc)\n",
    "    return (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "#FYI - Stop words that will be deleted by the remove_stops function:\n",
    "stops = stopwords.words(\"english\")\n",
    "print(stops)\n",
    "print(len(stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Led Woody toys live happily room birthday bring onto scene Afraid lose place heart Woody plots But circumstance separate Woody owner duo eventually learn put aside difference ', 'When sibling discover enchanted board game open door magical world unwittingly invite adult trap inside game year living room hope freedom finish game prove risky three find run giant rhinoceros evil monkey terrifying creature ', 'A family wedding reignite ancient feud nextdoor neighbor fishing buddy Meanwhile sultry Italian divorcée open restaurant local bait shop alarm local worry ll scare fish away But less interested seafood cook hot time ', 'Cheated mistreat step woman hold breath wait elusive good man break string lessthanstellar lover Friends confidant Vannah talk determine find good way breathe ', 'Just recover daughter wedding receive news pregnant wife expect He plan sell home plan like change arrival grandchild kid ']\n"
     ]
    }
   ],
   "source": [
    "#[10 mins to run] Get the cleaned overviews that will be fed into the TF-IDF function\n",
    "cleaned_overviews = clean_docs(overviews)\n",
    "print(cleaned_overviews[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate vectorizer model. Takes about 11 seconds\n",
    "vectorizer = TfidfVectorizer(\n",
    "                                lowercase=True,\n",
    "                                max_features= 5000,\n",
    "                                max_df=0.8,\n",
    "                                min_df=5,\n",
    "                                ngram_range = (1,3),\n",
    "                                stop_words = \"english\"\n",
    "\n",
    "                            )\n",
    "\n",
    "vectors = vectorizer.fit_transform(cleaned_overviews)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       abandon  abandoned  abduct  ability  able  aboard  abortion  abroad  \\\n",
      "0          0.0        0.0     0.0      0.0   0.0     0.0       0.0     0.0   \n",
      "1          0.0        0.0     0.0      0.0   0.0     0.0       0.0     0.0   \n",
      "2          0.0        0.0     0.0      0.0   0.0     0.0       0.0     0.0   \n",
      "3          0.0        0.0     0.0      0.0   0.0     0.0       0.0     0.0   \n",
      "4          0.0        0.0     0.0      0.0   0.0     0.0       0.0     0.0   \n",
      "...        ...        ...     ...      ...   ...     ...       ...     ...   \n",
      "45461      0.0        0.0     0.0      0.0   0.0     0.0       0.0     0.0   \n",
      "45462      0.0        0.0     0.0      0.0   0.0     0.0       0.0     0.0   \n",
      "45463      0.0        0.0     0.0      0.0   0.0     0.0       0.0     0.0   \n",
      "45464      0.0        0.0     0.0      0.0   0.0     0.0       0.0     0.0   \n",
      "45465      0.0        0.0     0.0      0.0   0.0     0.0       0.0     0.0   \n",
      "\n",
      "       abruptly  absence  ...  young woman  younger  youngster  youth  \\\n",
      "0           0.0      0.0  ...          0.0      0.0        0.0    0.0   \n",
      "1           0.0      0.0  ...          0.0      0.0        0.0    0.0   \n",
      "2           0.0      0.0  ...          0.0      0.0        0.0    0.0   \n",
      "3           0.0      0.0  ...          0.0      0.0        0.0    0.0   \n",
      "4           0.0      0.0  ...          0.0      0.0        0.0    0.0   \n",
      "...         ...      ...  ...          ...      ...        ...    ...   \n",
      "45461       0.0      0.0  ...          0.0      0.0        0.0    0.0   \n",
      "45462       0.0      0.0  ...          0.0      0.0        0.0    0.0   \n",
      "45463       0.0      0.0  ...          0.0      0.0        0.0    0.0   \n",
      "45464       0.0      0.0  ...          0.0      0.0        0.0    0.0   \n",
      "45465       0.0      0.0  ...          0.0      0.0        0.0    0.0   \n",
      "\n",
      "       youthful  zatoichi  zealand  zombie  zone  zoo  \n",
      "0           0.0       0.0      0.0     0.0   0.0  0.0  \n",
      "1           0.0       0.0      0.0     0.0   0.0  0.0  \n",
      "2           0.0       0.0      0.0     0.0   0.0  0.0  \n",
      "3           0.0       0.0      0.0     0.0   0.0  0.0  \n",
      "4           0.0       0.0      0.0     0.0   0.0  0.0  \n",
      "...         ...       ...      ...     ...   ...  ...  \n",
      "45461       0.0       0.0      0.0     0.0   0.0  0.0  \n",
      "45462       0.0       0.0      0.0     0.0   0.0  0.0  \n",
      "45463       0.0       0.0      0.0     0.0   0.0  0.0  \n",
      "45464       0.0       0.0      0.0     0.0   0.0  0.0  \n",
      "45465       0.0       0.0      0.0     0.0   0.0  0.0  \n",
      "\n",
      "[45466 rows x 5000 columns]\n"
     ]
    }
   ],
   "source": [
    "dense_vectors = vectors.toarray()\n",
    "df = pd.DataFrame(dense_vectors, columns=feature_names)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thing          0.326274\n",
      "man make       0.288789\n",
      "yellow         0.282622\n",
      "make friend    0.273409\n",
      "wicked         0.269847\n",
      "lion           0.266609\n",
      "make way       0.247814\n",
      "make           0.244373\n",
      "witch          0.230557\n",
      "magical        0.223757\n",
      "Name: 892, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Top values from TF-IDF tester\n",
    "\n",
    "top_values = df.iloc[892].sort_values(ascending=False)[:10]\n",
    "print(top_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1194)\t0.2699305269195095\n",
      "  (0, 240)\t0.3086276239876073\n",
      "  (0, 2481)\t0.18587933355357103\n",
      "  (0, 1492)\t0.21911256371901605\n",
      "  (0, 1332)\t0.27882724925029095\n",
      "  (0, 3150)\t0.21142878669386758\n",
      "  (0, 3918)\t0.24550681415803086\n",
      "  (0, 715)\t0.2614000968288274\n",
      "  (0, 2032)\t0.20934670381420872\n",
      "  (0, 3271)\t0.1825649931923838\n",
      "  (0, 2598)\t0.18073770574407927\n",
      "  (0, 86)\t0.29548835408556207\n",
      "  (0, 3840)\t0.21961735653572254\n",
      "  (0, 513)\t0.17759551521439162\n",
      "  (0, 428)\t0.2592311502134675\n",
      "  (0, 3766)\t0.22932245261784862\n",
      "  (0, 1994)\t0.280089595775914\n",
      "  (0, 2555)\t0.15207718971230008\n",
      "  (1, 986)\t0.17053828436053783\n",
      "  (1, 4423)\t0.20975821986765586\n",
      "  (1, 2857)\t0.23210543873502254\n",
      "  (1, 1497)\t0.14654184637445242\n",
      "  (1, 1882)\t0.1833631053916531\n",
      "  (1, 3782)\t0.1293226747954075\n",
      "  (1, 3733)\t0.24166340432981923\n",
      "  :\t:\n",
      "  (8, 1625)\t0.09833464947821229\n",
      "  (8, 3335)\t0.1734037653669758\n",
      "  (8, 1024)\t0.1977971464318253\n",
      "  (8, 4181)\t0.21009246792756595\n",
      "  (8, 312)\t0.17530337472957144\n",
      "  (8, 3935)\t0.19987707280674943\n",
      "  (8, 4472)\t0.1538610906294529\n",
      "  (8, 4336)\t0.20402349619120358\n",
      "  (8, 4387)\t0.11961291173699835\n",
      "  (8, 4311)\t0.19349010643133044\n",
      "  (8, 42)\t0.13705892667043948\n",
      "  (8, 2311)\t0.14836411612260572\n",
      "  (8, 1418)\t0.10921287150052102\n",
      "  (8, 3274)\t0.11434637814488084\n",
      "  (8, 1062)\t0.2131319650179467\n",
      "  (8, 1850)\t0.3960886485841673\n",
      "  (9, 518)\t0.3858913264739424\n",
      "  (9, 3707)\t0.28131729038807235\n",
      "  (9, 1173)\t0.38091115574122547\n",
      "  (9, 4832)\t0.3401626278842511\n",
      "  (9, 2474)\t0.289503087909166\n",
      "  (9, 3389)\t0.33713325611847733\n",
      "  (9, 4358)\t0.4267287456179957\n",
      "  (9, 2024)\t0.26194531457218334\n",
      "  (9, 2914)\t0.2509201144799701\n"
     ]
    }
   ],
   "source": [
    "print(vectors[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This calculates Cosines similarity between 2 vectors (movies).\n",
    "\n",
    "#Note: Cosine similarity expects 2D matrices. \n",
    "#To do cosine similarity on vectors, remember to reshape the vector in the  shape (1, N), where N is the vector length.\n",
    "def get_cosine_similarity(movie_vector_1, movie_vector_2):\n",
    "\n",
    "    cosine_sim = cosine_similarity(movie_vector_1, movie_vector_2)\n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03847733]]\n",
      "[[0.0178446]]\n",
      "[[0.00891219]]\n"
     ]
    }
   ],
   "source": [
    "#Testing Cosine Similarity\n",
    "\n",
    "movie_vector_1 = vectors[0] #Toy Story\n",
    "movie_vector_2 = vectors[1] #Jumanji\n",
    "\n",
    "print(get_cosine_similarity(movie_vector_1, movie_vector_2))\n",
    "\n",
    "movie_vector_1 = vectors[4766] #Harry Potter 1 (TPS)\n",
    "movie_vector_2 = vectors[5678] #Harry Potter 2 (TCoS)\n",
    "\n",
    "print(get_cosine_similarity(movie_vector_1, movie_vector_2))\n",
    "\n",
    "movie_vector_1 = vectors[4766] #Harry Potter 1 (TPS)\n",
    "movie_vector_2 = vectors[892] #The Wizard of Oz\n",
    "print(get_cosine_similarity(movie_vector_1, movie_vector_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_movies_cosine(tfidf_matrix, movie_index, movie_titles, top_n=5):\n",
    "    \n",
    "    # Compute cosine similarity between the movie at movie_index and all movies in the matrix\n",
    "    cosine_similarities = cosine_similarity(tfidf_matrix[movie_index], tfidf_matrix).flatten()\n",
    "    \n",
    "    # Get the indices of the top_n movies with the highest cosine similarity scores\n",
    "    # Use argsort and reverse it with [::-1] to get the indices in descending order of similarity\n",
    "    # Skip the first one as it is the movie itself with a similarity of 1\n",
    "    similar_indices = cosine_similarities.argsort()[::-1][1:top_n+1]\n",
    "    \n",
    "    # Get the scores for the top_n movies\n",
    "    similar_scores = cosine_similarities[similar_indices]\n",
    "    \n",
    "    # Combine indices and scores into a list of tuples and return\n",
    "    top_movies = [(movie_titles[index], index, score) for index, score in zip(similar_indices, similar_scores)]\n",
    "\n",
    "    print(f\"Top similar movies to {titles[movie_index]}:\\n\")\n",
    "    for num, (title, index, score) in enumerate(top_movies, start = 1):\n",
    "        print(f\"{num}. \\\"{title}\\\" at ROW {index} with similarity score: {score}\")\n",
    "\n",
    "    return top_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top similar movies to Die Hard: With a Vengeance:\n",
      "\n",
      "1. \"Broadway Melody of 1940\" at ROW 10175 with similarity score: 0.3140144503354083\n",
      "2. \"Loose Cannons\" at ROW 6411 with similarity score: 0.2807426588650285\n",
      "3. \"If These Knishes Could Talk: The Story of the NY Accent\" at ROW 37657 with similarity score: 0.26426619815728375\n",
      "4. \"The Transfiguration\" at ROW 43275 with similarity score: 0.26344914917684265\n",
      "5. \"Khiladi 786\" at ROW 40343 with similarity score: 0.2590249725437656\n",
      "6. \"Les Ripoux\" at ROW 44766 with similarity score: 0.2573206898414055\n",
      "7. \"Texas Killing Fields\" at ROW 19018 with similarity score: 0.23704803406060815\n",
      "8. \"Strictly Ballroom\" at ROW 1147 with similarity score: 0.2257942291751761\n",
      "9. \"Shoot the Moon\" at ROW 5989 with similarity score: 0.21235341819954417\n",
      "10. \"Strike Force\" at ROW 31825 with similarity score: 0.21160490910240262\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Broadway Melody of 1940', 10175, 0.3140144503354083),\n",
       " ('Loose Cannons', 6411, 0.2807426588650285),\n",
       " ('If These Knishes Could Talk: The Story of the NY Accent',\n",
       "  37657,\n",
       "  0.26426619815728375),\n",
       " ('The Transfiguration', 43275, 0.26344914917684265),\n",
       " ('Khiladi 786', 40343, 0.2590249725437656),\n",
       " ('Les Ripoux', 44766, 0.2573206898414055),\n",
       " ('Texas Killing Fields', 19018, 0.23704803406060815),\n",
       " ('Strictly Ballroom', 1147, 0.2257942291751761),\n",
       " ('Shoot the Moon', 5989, 0.21235341819954417),\n",
       " ('Strike Force', 31825, 0.21160490910240262)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_movies_cosine(vectors, 162, titles, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Top 5 similar movies to movie {movie_index}:\\n\")\n",
    "for index, score in similar_movies:\n",
    "    print(f\"Movie {index} with similarity score: {score}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
